---
phase: 06-fact-extraction-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - osint_system/agents/sifters/base_sifter.py
  - osint_system/agents/sifters/fact_extraction_agent.py
  - osint_system/config/prompts/__init__.py
  - osint_system/config/prompts/fact_extraction_prompts.py
  - tests/agents/sifters/test_fact_extraction_agent.py
autonomous: true

must_haves:
  truths:
    - "Agent extracts facts from raw article text"
    - "Output conforms to ExtractedFact schema"
    - "Entity markers appear in claim text with [E1:name] format"
    - "Extraction confidence and claim clarity are separate scores"
    - "Denials produce underlying claim with assertion_type='denial'"
    - "Quoted speech produces nested facts"
    - "Agent handles empty/malformed input gracefully"
  artifacts:
    - path: "osint_system/agents/sifters/base_sifter.py"
      provides: "BaseSifter abstract class inheriting from BaseAgent"
      exports: ["BaseSifter"]
      min_lines: 40
    - path: "osint_system/agents/sifters/fact_extraction_agent.py"
      provides: "FactExtractionAgent with Gemini-powered extraction"
      exports: ["FactExtractionAgent"]
      min_lines: 200
    - path: "osint_system/config/prompts/fact_extraction_prompts.py"
      provides: "Prompt templates for fact extraction"
      exports: ["FACT_EXTRACTION_SYSTEM_PROMPT", "FACT_EXTRACTION_USER_PROMPT"]
      min_lines: 100
  key_links:
    - from: "osint_system/agents/sifters/fact_extraction_agent.py"
      to: "osint_system/data_management/schemas/fact_schema.py"
      via: "ExtractedFact import for output validation"
      pattern: "from.*schemas.*import.*ExtractedFact"
    - from: "osint_system/agents/sifters/fact_extraction_agent.py"
      to: "osint_system/config/prompts/fact_extraction_prompts.py"
      via: "Prompt template import"
      pattern: "from.*prompts.*import"
    - from: "osint_system/agents/sifters/fact_extraction_agent.py"
      to: "Gemini API"
      via: "google.generativeai client call"
      pattern: "model\\.generate_content"
---

<objective>
Implement FactExtractionAgent with optimized prompts that produce ExtractedFact-conformant output.

Purpose: This is the core extraction engine. The LLM receives raw text and produces structured facts following CONTEXT.md decisions. Prompt engineering is critical for output quality.

Output: Working FactExtractionAgent that accepts article content and returns list[ExtractedFact].
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-fact-extraction-pipeline/06-CONTEXT.md
@.planning/phases/06-fact-extraction-pipeline/06-01-SUMMARY.md

# Agent patterns
@osint_system/agents/base_agent.py
@osint_system/agents/planning_agent.py

# Schema from Plan 01
@osint_system/data_management/schemas/fact_schema.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement BaseSifter and prompt templates</name>
  <files>
    osint_system/agents/sifters/base_sifter.py
    osint_system/config/prompts/__init__.py
    osint_system/config/prompts/fact_extraction_prompts.py
  </files>
  <action>
**base_sifter.py:**
```python
"""Base class for sifter agents that process and analyze crawled content."""

from abc import abstractmethod
from typing import Any
from osint_system.agents.base_agent import BaseAgent


class BaseSifter(BaseAgent):
    """
    Abstract base for sifter agents.

    Sifters receive raw content from crawlers and produce structured
    intelligence outputs (facts, classifications, verifications).

    Unlike crawlers that acquire data, sifters analyze and transform it.
    """

    def __init__(self, name: str, description: str = "", **kwargs):
        super().__init__(name=name, description=description, **kwargs)
        self.processed_count = 0
        self.error_count = 0

    @abstractmethod
    async def sift(self, content: dict) -> list[dict]:
        """
        Process content and extract structured output.

        Args:
            content: Raw content dict with 'text', 'source_id', 'metadata'

        Returns:
            List of extracted items (facts, classifications, etc.)
        """
        pass

    async def process(self, input_data: dict) -> dict:
        """
        BaseAgent.process implementation routing to sift().

        Args:
            input_data: Dict with 'content' key containing text to process

        Returns:
            Dict with 'results' list and status
        """
        try:
            results = await self.sift(input_data.get("content", {}))
            self.processed_count += 1
            return {
                "success": True,
                "results": results,
                "count": len(results)
            }
        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Sift failed: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "results": []
            }

    def get_capabilities(self) -> list[str]:
        return ["sifting", "analysis"]
```

**prompts/__init__.py:** Create directory and init.

**fact_extraction_prompts.py:**
```python
"""Prompt templates for fact extraction per Phase 6 CONTEXT.md."""

FACT_EXTRACTION_SYSTEM_PROMPT = '''You are an expert OSINT fact extractor. Your task is to identify discrete, verifiable facts from source text.

## Extraction Rules

### Fact Granularity
- Extract single subject-predicate-object assertions, not maximally decomposed atoms
- "Putin visited Beijing in March 2024" = one fact, not four
- Extract notable entities even without explicit assertions

### Entity Marking
- Mark entities in claim text: [E1:Putin] visited [E2:Beijing] in [T1:March 2024]
- Use E1, E2, E3... for entities
- Use T1, T2, T3... for temporal markers
- Provide separate entity objects with type, canonical form, and cluster_id if aliases exist

### Handling Special Cases
- **Denials**: "Russia denied X" -> extract claim X with assertion_type="denial", asserter="Russia"
- **Quoted speech**: "Official said Y" -> extract BOTH the statement event AND the underlying claim Y
- **Implicit facts**: "The late President X" -> extract "X is deceased" with extraction_type="inferred"
- **Predictions**: "Russia plans to..." -> extract with claim_type="prediction"
- **Hedged language**: "allegedly", "reportedly" -> reduce claim_clarity score

### Confidence Scoring
- extraction_confidence: Your parsing accuracy (0.0-1.0). How confident you correctly understood the text?
- claim_clarity: Source text ambiguity (0.0-1.0). How clear/unambiguous is the source statement?
These are SEPARATE dimensions. A vague claim ("sources say") can be accurately extracted (high extraction, low clarity).

### Provenance
- Preserve attribution chains: who said what to whom
- Note hop_count: 0 = eyewitness, 1 = quoting eyewitness, etc.
- Keep attribution_phrase verbatim: "according to Reuters citing officials"

### Output Format
Return a JSON array of fact objects. Each object must have:
- claim: {text (with entity markers), assertion_type, claim_type}
- entities: [{id, text, type, canonical}]
- quality: {extraction_confidence, claim_clarity}
- provenance: {quote, offsets (if available)}

Optional but valuable:
- temporal: {id, value, precision, temporal_precision}
- relationships: [{type, target_fact_id, confidence}]
'''

FACT_EXTRACTION_USER_PROMPT = '''Extract facts from the following source text.

SOURCE_ID: {source_id}
SOURCE_TYPE: {source_type}
PUBLICATION_DATE: {publication_date}

---TEXT START---
{text}
---TEXT END---

Extract all discrete, verifiable facts. Return ONLY a JSON array of fact objects, no other text.'''

# For processing very long documents
FACT_EXTRACTION_CHUNK_PROMPT = '''Continue extracting facts from this chunk. Maintain entity ID continuity from previous chunks.

Previous entities: {previous_entities}
Previous fact count: {previous_count}

---CHUNK {chunk_num} START---
{text}
---CHUNK END---

Return ONLY a JSON array of new fact objects.'''
```
  </action>
  <verify>
```bash
uv run python -c "
from osint_system.agents.sifters.base_sifter import BaseSifter
from osint_system.config.prompts.fact_extraction_prompts import (
    FACT_EXTRACTION_SYSTEM_PROMPT,
    FACT_EXTRACTION_USER_PROMPT
)
print(f'System prompt: {len(FACT_EXTRACTION_SYSTEM_PROMPT)} chars')
print(f'User prompt template has source_id: {\"source_id\" in FACT_EXTRACTION_USER_PROMPT}')
print('Imports OK')
"
```
  </verify>
  <done>BaseSifter class and prompt templates ready for FactExtractionAgent</done>
</task>

<task type="auto">
  <name>Task 2a: Implement FactExtractionAgent core structure and Gemini client</name>
  <files>osint_system/agents/sifters/fact_extraction_agent.py</files>
  <action>
Create the FactExtractionAgent class with core structure and Gemini client setup.

**fact_extraction_agent.py (Part 1 - Core Structure):**
```python
"""Fact extraction agent using Gemini for structured fact identification."""

import json
import re
from typing import Optional, Any
from datetime import datetime

from loguru import logger

from osint_system.agents.sifters.base_sifter import BaseSifter
from osint_system.data_management.schemas import (
    ExtractedFact, Claim, Entity, Provenance,
    QualityMetrics, ExtractionMetadata, TemporalMarker,
    SourceType, EntityType
)
from osint_system.config.prompts.fact_extraction_prompts import (
    FACT_EXTRACTION_SYSTEM_PROMPT,
    FACT_EXTRACTION_USER_PROMPT,
    FACT_EXTRACTION_CHUNK_PROMPT,
)
from osint_system.config.settings import settings


class FactExtractionAgent(BaseSifter):
    """
    Extracts structured facts from raw text using Gemini.

    Produces ExtractedFact objects conforming to Phase 6 schema with:
    - Entity-marked claim text
    - Separate extraction_confidence and claim_clarity
    - Full provenance with attribution chains
    - Relationship hints between facts

    Attributes:
        model_name: Gemini model to use (default: gemini-1.5-flash)
        chunk_size: Max characters per extraction chunk
        min_confidence: Minimum extraction_confidence to keep fact
    """

    # Chunk size for long documents (leave room for prompt)
    DEFAULT_CHUNK_SIZE = 12000

    def __init__(
        self,
        model_name: str = "gemini-1.5-flash",
        chunk_size: int = DEFAULT_CHUNK_SIZE,
        min_confidence: float = 0.0,  # Default: include all per CONTEXT.md
        gemini_client: Optional[Any] = None,
    ):
        super().__init__(
            name="FactExtractionAgent",
            description="Extracts structured facts from text using LLM"
        )
        self.model_name = model_name
        self.chunk_size = chunk_size
        self.min_confidence = min_confidence
        self.gemini_client = gemini_client or self._get_gemini_client()

        self.logger.info(
            "FactExtractionAgent initialized",
            model=model_name,
            chunk_size=chunk_size,
            min_confidence=min_confidence
        )

    def _get_gemini_client(self):
        """Initialize Gemini client."""
        try:
            import google.generativeai as genai
            genai.configure(api_key=settings.gemini_api_key)
            return genai
        except Exception as e:
            self.logger.warning(f"Failed to initialize Gemini: {e}")
            return None

    async def sift(self, content: dict) -> list[dict]:
        """
        Extract facts from content.

        Args:
            content: {
                'text': str,
                'source_id': str,
                'source_type': str (optional),
                'publication_date': str (optional),
                'metadata': dict (optional)
            }

        Returns:
            List of ExtractedFact as dicts
        """
        text = content.get("text", "")
        source_id = content.get("source_id", "unknown")
        source_type = content.get("source_type", "unknown")
        pub_date = content.get("publication_date", "")

        if not text or len(text.strip()) < 50:
            self.logger.warning("Text too short for extraction", length=len(text))
            return []

        # Handle long documents with chunking
        if len(text) > self.chunk_size:
            return await self._extract_chunked(text, source_id, source_type, pub_date)

        return await self._extract_single(text, source_id, source_type, pub_date)

    async def _extract_single(
        self,
        text: str,
        source_id: str,
        source_type: str,
        pub_date: str
    ) -> list[dict]:
        """Extract facts from a single text chunk."""
        if not self.gemini_client:
            self.logger.error("Gemini client not available")
            return []

        try:
            model = self.gemini_client.GenerativeModel(
                self.model_name,
                system_instruction=FACT_EXTRACTION_SYSTEM_PROMPT
            )

            prompt = FACT_EXTRACTION_USER_PROMPT.format(
                source_id=source_id,
                source_type=source_type,
                publication_date=pub_date or "unknown",
                text=text
            )

            response = model.generate_content(prompt)
            raw_json = self._extract_json_from_response(response.text)

            if not raw_json:
                self.logger.warning("No valid JSON in response")
                return []

            facts = self._parse_and_validate(raw_json, source_id)

            self.logger.info(
                f"Extracted {len(facts)} facts",
                source_id=source_id,
                text_length=len(text)
            )

            return [f.model_dump() for f in facts]

        except Exception as e:
            self.logger.error(f"Extraction failed: {e}", exc_info=True)
            return []

    def get_capabilities(self) -> list[str]:
        return [
            "fact_extraction",
            "entity_extraction",
            "claim_identification",
            "confidence_scoring",
            "provenance_tracking"
        ]
```

Update osint_system/agents/sifters/__init__.py to export FactExtractionAgent.
  </action>
  <verify>
```bash
uv run python -c "
from osint_system.agents.sifters import FactExtractionAgent
agent = FactExtractionAgent(gemini_client=None)  # No API call
print(f'Agent: {agent.name}')
print(f'Capabilities: {agent.get_capabilities()}')
print(f'Chunk size: {agent.chunk_size}')
print('Agent core structure OK')
"
```
  </verify>
  <done>FactExtractionAgent core structure with Gemini client setup complete</done>
</task>

<task type="auto">
  <name>Task 2b: Implement chunking strategy for long documents</name>
  <files>osint_system/agents/sifters/fact_extraction_agent.py</files>
  <action>
Add chunking methods to FactExtractionAgent for handling long documents.

**Add these methods to FactExtractionAgent class:**

```python
    async def _extract_chunked(
        self,
        text: str,
        source_id: str,
        source_type: str,
        pub_date: str
    ) -> list[dict]:
        """Extract from long text using chunking with entity continuity."""
        chunks = self._split_into_chunks(text)
        all_facts = []
        previous_entities = []

        self.logger.info(f"Processing {len(chunks)} chunks for {source_id}")

        for i, chunk in enumerate(chunks):
            if i == 0:
                # First chunk uses standard prompt
                chunk_facts = await self._extract_single(
                    chunk, source_id, source_type, pub_date
                )
            else:
                # Subsequent chunks maintain entity continuity
                chunk_facts = await self._extract_continuation(
                    chunk, source_id, previous_entities, len(all_facts), i + 1
                )

            all_facts.extend(chunk_facts)

            # Update entity list for continuity
            for fact in chunk_facts:
                if isinstance(fact, dict) and "entities" in fact:
                    previous_entities.extend(fact["entities"])

        return all_facts

    async def _extract_continuation(
        self,
        text: str,
        source_id: str,
        previous_entities: list,
        previous_count: int,
        chunk_num: int
    ) -> list[dict]:
        """Extract from continuation chunk with entity context."""
        if not self.gemini_client:
            return []

        try:
            model = self.gemini_client.GenerativeModel(
                self.model_name,
                system_instruction=FACT_EXTRACTION_SYSTEM_PROMPT
            )

            # Format previous entities for context
            entity_summary = ", ".join([
                f"{e.get('id')}: {e.get('canonical', e.get('text'))}"
                for e in previous_entities[-10:]  # Last 10 entities
            ])

            prompt = FACT_EXTRACTION_CHUNK_PROMPT.format(
                previous_entities=entity_summary or "none",
                previous_count=previous_count,
                chunk_num=chunk_num,
                text=text
            )

            response = model.generate_content(prompt)
            raw_json = self._extract_json_from_response(response.text)

            if not raw_json:
                return []

            facts = self._parse_and_validate(raw_json, source_id)
            return [f.model_dump() for f in facts]

        except Exception as e:
            self.logger.error(f"Chunk extraction failed: {e}")
            return []

    def _split_into_chunks(self, text: str) -> list[str]:
        """Split text into chunks, preferring paragraph boundaries."""
        if len(text) <= self.chunk_size:
            return [text]

        chunks = []
        paragraphs = text.split('\n\n')
        current_chunk = ""

        for para in paragraphs:
            if len(current_chunk) + len(para) + 2 <= self.chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                # Handle paragraphs larger than chunk_size
                if len(para) > self.chunk_size:
                    # Split by sentences
                    sentences = re.split(r'(?<=[.!?])\s+', para)
                    current_chunk = ""
                    for sent in sentences:
                        if len(current_chunk) + len(sent) + 1 <= self.chunk_size:
                            current_chunk += sent + " "
                        else:
                            if current_chunk:
                                chunks.append(current_chunk.strip())
                            current_chunk = sent + " "
                else:
                    current_chunk = para + "\n\n"

        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return chunks
```
  </action>
  <verify>
```bash
uv run python -c "
from osint_system.agents.sifters import FactExtractionAgent
agent = FactExtractionAgent(gemini_client=None)

# Test chunk splitting
long_text = ('This is a paragraph.\n\n' * 100)
chunks = agent._split_into_chunks(long_text)
print(f'Split {len(long_text)} chars into {len(chunks)} chunks')

# Verify no chunk exceeds limit
max_chunk = max(len(c) for c in chunks)
print(f'Max chunk size: {max_chunk} (limit: {agent.chunk_size})')
assert max_chunk <= agent.chunk_size, 'Chunk exceeds limit'
print('Chunking strategy OK')
"
```
  </verify>
  <done>Chunking strategy handles long documents with entity continuity</done>
</task>

<task type="auto">
  <name>Task 2c: Implement validation and conversion logic</name>
  <files>osint_system/agents/sifters/fact_extraction_agent.py</files>
  <action>
Add JSON extraction, parsing, and validation methods to FactExtractionAgent.

**Add these methods to FactExtractionAgent class:**

```python
    def _extract_json_from_response(self, response_text: str) -> Optional[list]:
        """Extract JSON array from LLM response, handling markdown blocks."""
        text = response_text.strip()

        # Try to find JSON in markdown code block
        json_match = re.search(r'```(?:json)?\s*([\s\S]*?)```', text)
        if json_match:
            text = json_match.group(1).strip()

        # Try to find array directly
        array_match = re.search(r'\[[\s\S]*\]', text)
        if array_match:
            text = array_match.group(0)

        try:
            parsed = json.loads(text)
            if isinstance(parsed, list):
                return parsed
            elif isinstance(parsed, dict):
                # Sometimes LLM wraps in object
                return [parsed]
            return None
        except json.JSONDecodeError:
            self.logger.warning("Failed to parse JSON from response")
            return None

    def _parse_and_validate(
        self,
        raw_facts: list,
        source_id: str
    ) -> list[ExtractedFact]:
        """Parse raw JSON and validate into ExtractedFact objects."""
        validated = []

        for raw in raw_facts:
            try:
                fact = self._raw_to_extracted_fact(raw, source_id)

                # Apply minimum confidence filter
                if fact.quality and fact.quality.extraction_confidence < self.min_confidence:
                    continue

                validated.append(fact)

            except Exception as e:
                self.logger.debug(f"Fact validation failed: {e}", raw=raw)
                continue

        return validated

    def _raw_to_extracted_fact(self, raw: dict, source_id: str) -> ExtractedFact:
        """Convert raw LLM output to ExtractedFact."""
        # Extract claim
        claim_data = raw.get("claim", {})
        if isinstance(claim_data, str):
            claim_data = {"text": claim_data}

        claim = Claim(
            text=claim_data.get("text", raw.get("text", "")),
            assertion_type=claim_data.get("assertion_type", "statement"),
            claim_type=claim_data.get("claim_type", "event")
        )

        # Extract entities
        entities = []
        for ent in raw.get("entities", []):
            try:
                entity_type = ent.get("type", "PERSON")
                if isinstance(entity_type, str):
                    entity_type = EntityType(entity_type.upper())
                entities.append(Entity(
                    id=ent.get("id", f"E{len(entities)+1}"),
                    text=ent.get("text", ""),
                    type=entity_type,
                    canonical=ent.get("canonical"),
                    cluster_id=ent.get("cluster_id")
                ))
            except Exception:
                continue

        # Extract quality metrics
        quality = None
        quality_data = raw.get("quality", {})
        if quality_data:
            quality = QualityMetrics(
                extraction_confidence=float(quality_data.get("extraction_confidence", 0.8)),
                claim_clarity=float(quality_data.get("claim_clarity", 0.8))
            )

        # Extract provenance
        provenance = None
        prov_data = raw.get("provenance", {})
        if prov_data:
            provenance = Provenance(
                source_id=source_id,
                quote=prov_data.get("quote", claim.text),
                offsets=prov_data.get("offsets", {"start": 0, "end": 0}),
                hop_count=prov_data.get("hop_count", 1),
                source_type=SourceType(prov_data.get("source_type", "unknown").lower())
                    if prov_data.get("source_type") else SourceType.UNKNOWN
            )

        # Extract temporal
        temporal = None
        temp_data = raw.get("temporal", {})
        if temp_data and temp_data.get("value"):
            temporal = TemporalMarker(
                id=temp_data.get("id", "T1"),
                value=temp_data.get("value"),
                precision=temp_data.get("precision", "day"),
                temporal_precision=temp_data.get("temporal_precision", "unknown")
            )

        return ExtractedFact(
            claim=claim,
            entities=entities,
            quality=quality,
            provenance=provenance,
            temporal=temporal,
            extraction=ExtractionMetadata(
                model_version=self.model_name,
                extraction_type=raw.get("extraction", {}).get("extraction_type", "explicit")
            )
        )
```
  </action>
  <verify>
```bash
uv run python -c "
from osint_system.agents.sifters import FactExtractionAgent
agent = FactExtractionAgent(gemini_client=None)

# Test JSON extraction
test_responses = [
    '[{\"claim\": {\"text\": \"Test\"}}]',
    '\`\`\`json\n[{\"claim\": {\"text\": \"Test\"}}]\n\`\`\`',
    'Here are the facts: [{\"claim\": {\"text\": \"Test\"}}]',
]

for resp in test_responses:
    result = agent._extract_json_from_response(resp)
    assert result is not None, f'Failed to parse: {resp[:30]}'
    print(f'Parsed OK: {resp[:30]}...')

# Test raw-to-fact conversion
raw = {
    'claim': {'text': '[E1:Putin] visited Beijing', 'assertion_type': 'statement'},
    'entities': [{'id': 'E1', 'text': 'Putin', 'type': 'PERSON'}],
    'quality': {'extraction_confidence': 0.9, 'claim_clarity': 0.85}
}
fact = agent._raw_to_extracted_fact(raw, 'test-source')
print(f'Converted fact: {fact.claim.text}')
print(f'Entities: {len(fact.entities)}')
print('Validation/conversion logic OK')
"
```
  </verify>
  <done>JSON extraction, parsing, and validation logic complete</done>
</task>

<task type="auto">
  <name>Task 3: Add extraction agent tests</name>
  <files>tests/agents/sifters/test_fact_extraction_agent.py</files>
  <action>
Create comprehensive tests for FactExtractionAgent.

**Tests to include:**
1. Agent initialization and capabilities
2. JSON extraction from various response formats (plain, markdown block)
3. Raw-to-ExtractedFact conversion
4. Empty/short text handling
5. Chunk splitting logic
6. Entity parsing with various types
7. Denial assertion handling
8. Mock Gemini response end-to-end

Create tests/agents/sifters/__init__.py (empty if needed).

Use unittest.mock to mock Gemini responses - do NOT make actual API calls.

Example mock pattern:
```python
from unittest.mock import MagicMock, patch

@pytest.fixture
def mock_gemini():
    with patch('osint_system.agents.sifters.fact_extraction_agent.settings') as mock_settings:
        mock_settings.gemini_api_key = "test-key"
        yield mock_settings

def test_extraction_with_mock_response(mock_gemini):
    mock_response = MagicMock()
    mock_response.text = '''[
        {
            "claim": {"text": "[E1:Putin] visited [E2:Beijing]", "assertion_type": "statement"},
            "entities": [
                {"id": "E1", "text": "Putin", "type": "PERSON", "canonical": "Vladimir Putin"},
                {"id": "E2", "text": "Beijing", "type": "LOCATION"}
            ],
            "quality": {"extraction_confidence": 0.9, "claim_clarity": 0.85}
        }
    ]'''
    # ... test extraction
```
  </action>
  <verify>
`uv run python -m pytest tests/agents/sifters/test_fact_extraction_agent.py -v`
  </verify>
  <done>All agent tests pass including mock Gemini integration</done>
</task>

</tasks>

<verification>
```bash
# All imports work
uv run python -c "
from osint_system.agents.sifters import BaseSifter, FactExtractionAgent
from osint_system.config.prompts.fact_extraction_prompts import FACT_EXTRACTION_SYSTEM_PROMPT
print('All imports OK')
"

# Tests pass
uv run python -m pytest tests/agents/sifters/test_fact_extraction_agent.py -v

# Agent capabilities
uv run python -c "
from osint_system.agents.sifters import FactExtractionAgent
agent = FactExtractionAgent(gemini_client=None)
print('Capabilities:', agent.get_capabilities())
"
```
</verification>

<success_criteria>
- BaseSifter provides abstract interface for all sifter agents
- FactExtractionAgent extracts structured facts from text
- Output conforms to ExtractedFact schema (from Plan 01)
- Entity markers appear in claim text with [E1:name] format
- Denials produce underlying claim with assertion_type='denial'
- extraction_confidence and claim_clarity are separate scores
- Long documents handled via chunking with entity continuity
- Empty/malformed input returns empty list without crashing
- All tests pass with mocked Gemini responses
</success_criteria>

<output>
After completion, create `.planning/phases/06-fact-extraction-pipeline/06-02-SUMMARY.md`
</output>
