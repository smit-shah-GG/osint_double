---
phase: 06-fact-extraction-pipeline
plan: 03
type: execute
wave: 3
depends_on: ["06-02"]
files_modified:
  - osint_system/data_management/fact_store.py
  - osint_system/agents/sifters/fact_consolidator.py
  - tests/data_management/test_fact_store.py
  - tests/agents/sifters/test_fact_consolidator.py
autonomous: true

must_haves:
  truths:
    - "Exact duplicates detected by content hash"
    - "Semantic duplicates linked as variants above 0.3 threshold"
    - "Below 0.3 similarity, facts discarded as noise"
    - "Multiple sources for same claim preserved with provenance"
    - "Facts stored with investigation_id scoping"
    - "Fast O(1) lookup by fact_id and content_hash"
  artifacts:
    - path: "osint_system/data_management/fact_store.py"
      provides: "FactStore for investigation-scoped fact persistence"
      exports: ["FactStore"]
      min_lines: 150
    - path: "osint_system/agents/sifters/fact_consolidator.py"
      provides: "FactConsolidator for dedup and variant linking"
      exports: ["FactConsolidator"]
      min_lines: 120
  key_links:
    - from: "osint_system/agents/sifters/fact_consolidator.py"
      to: "osint_system/data_management/fact_store.py"
      via: "FactStore for persistence"
      pattern: "from.*fact_store.*import.*FactStore"
    - from: "osint_system/agents/sifters/fact_consolidator.py"
      to: "osint_system/data_management/schemas/fact_schema.py"
      via: "ExtractedFact schema"
      pattern: "from.*schemas.*import.*ExtractedFact"
---

<objective>
Implement deduplication, consolidation, and storage for extracted facts.

Purpose: Multiple sources may report the same claim. Deduplication prevents bloat while consolidation preserves the corroboration signal (three sources saying X is different from one source). Storage enables downstream phases to access facts efficiently.

Output: FactStore for persistence and FactConsolidator for dedup/linking, following CONTEXT.md decisions on thresholds (0.3 for semantic similarity).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-fact-extraction-pipeline/06-CONTEXT.md
@.planning/phases/06-fact-extraction-pipeline/06-01-SUMMARY.md
@.planning/phases/06-fact-extraction-pipeline/06-02-SUMMARY.md

# Storage patterns
@osint_system/data_management/article_store.py
@osint_system/agents/crawlers/deduplication/dedup_engine.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement FactStore</name>
  <files>osint_system/data_management/fact_store.py</files>
  <action>
Create investigation-scoped fact storage following ArticleStore patterns.

**fact_store.py:**
```python
"""Fact storage with investigation-scoped persistence and fast lookup."""

import asyncio
import json
from typing import Dict, List, Optional, Any
from datetime import datetime, timezone
from pathlib import Path
from loguru import logger

from osint_system.data_management.schemas import ExtractedFact


class FactStore:
    """
    Storage adapter for extracted facts with investigation-based organization.

    Features:
    - Investigation-scoped storage (investigation_id as primary key)
    - O(1) lookup by fact_id and content_hash
    - Variant linking for semantic duplicates
    - Optional JSON persistence for beta
    - Thread-safe operations with asyncio locks

    Data structure:
    {
        "investigation_id": {
            "metadata": {...},
            "facts": {
                "fact_id": ExtractedFact dict,
                ...
            },
            "hash_index": {
                "content_hash": ["fact_id", ...],
                ...
            },
            "source_index": {
                "source_id": ["fact_id", ...],
                ...
            }
        }
    }
    """

    def __init__(self, persistence_path: Optional[str] = None):
        """
        Initialize fact store.

        Args:
            persistence_path: Optional path to JSON file for persistence.
                            If None, storage is memory-only.
        """
        self._storage: Dict[str, Dict[str, Any]] = {}
        self._lock = asyncio.Lock()
        self.persistence_path = Path(persistence_path) if persistence_path else None
        self.logger = logger.bind(component="FactStore")

        if self.persistence_path and self.persistence_path.exists():
            self._load_from_file()

        self.logger.info(
            "FactStore initialized",
            persistence_enabled=self.persistence_path is not None
        )

    def _init_investigation(self, investigation_id: str) -> None:
        """Initialize storage structure for an investigation."""
        if investigation_id not in self._storage:
            self._storage[investigation_id] = {
                "metadata": {},
                "created_at": datetime.now(timezone.utc).isoformat(),
                "updated_at": datetime.now(timezone.utc).isoformat(),
                "facts": {},
                "hash_index": {},
                "source_index": {},
            }

    async def save_facts(
        self,
        investigation_id: str,
        facts: List[Dict[str, Any]],
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Save facts for an investigation.

        Handles dedup by content_hash - exact duplicates are skipped,
        existing facts updated with new variant links.

        Args:
            investigation_id: Investigation identifier
            facts: List of ExtractedFact dicts
            metadata: Optional investigation metadata

        Returns:
            Stats dict: {saved, updated, duplicates, total}
        """
        async with self._lock:
            self._init_investigation(investigation_id)

            inv = self._storage[investigation_id]
            if metadata:
                inv["metadata"].update(metadata)

            saved = 0
            updated = 0
            duplicates = 0

            for fact in facts:
                fact_id = fact.get("fact_id")
                content_hash = fact.get("content_hash")

                if not fact_id or not content_hash:
                    self.logger.warning("Fact missing id or hash, skipping")
                    continue

                # Check for exact duplicate by hash
                if content_hash in inv["hash_index"]:
                    existing_ids = inv["hash_index"][content_hash]
                    if fact_id in existing_ids:
                        duplicates += 1
                        continue
                    # Same content, different ID = add to variants
                    for existing_id in existing_ids:
                        existing_fact = inv["facts"].get(existing_id)
                        if existing_fact:
                            variants = existing_fact.get("variants", [])
                            if fact_id not in variants:
                                variants.append(fact_id)
                                existing_fact["variants"] = variants
                    updated += 1

                # Store fact
                inv["facts"][fact_id] = fact
                saved += 1

                # Update hash index
                if content_hash not in inv["hash_index"]:
                    inv["hash_index"][content_hash] = []
                if fact_id not in inv["hash_index"][content_hash]:
                    inv["hash_index"][content_hash].append(fact_id)

                # Update source index
                source_id = fact.get("provenance", {}).get("source_id")
                if source_id:
                    if source_id not in inv["source_index"]:
                        inv["source_index"][source_id] = []
                    inv["source_index"][source_id].append(fact_id)

            inv["updated_at"] = datetime.now(timezone.utc).isoformat()
            inv["fact_count"] = len(inv["facts"])

            if self.persistence_path:
                self._save_to_file()

            stats = {
                "saved": saved,
                "updated": updated,
                "duplicates": duplicates,
                "total": len(inv["facts"])
            }

            self.logger.info(f"Saved facts for {investigation_id}", **stats)
            return stats

    async def get_fact(
        self,
        investigation_id: str,
        fact_id: str
    ) -> Optional[Dict[str, Any]]:
        """Get a single fact by ID. O(1) lookup."""
        async with self._lock:
            inv = self._storage.get(investigation_id, {})
            return inv.get("facts", {}).get(fact_id)

    async def get_by_hash(
        self,
        investigation_id: str,
        content_hash: str
    ) -> List[Dict[str, Any]]:
        """Get all facts with matching content hash."""
        async with self._lock:
            inv = self._storage.get(investigation_id, {})
            fact_ids = inv.get("hash_index", {}).get(content_hash, [])
            return [
                inv["facts"][fid]
                for fid in fact_ids
                if fid in inv["facts"]
            ]

    async def get_by_source(
        self,
        investigation_id: str,
        source_id: str
    ) -> List[Dict[str, Any]]:
        """Get all facts from a specific source."""
        async with self._lock:
            inv = self._storage.get(investigation_id, {})
            fact_ids = inv.get("source_index", {}).get(source_id, [])
            return [
                inv["facts"][fid]
                for fid in fact_ids
                if fid in inv["facts"]
            ]

    async def get_all_facts(
        self,
        investigation_id: str,
        limit: Optional[int] = None,
        offset: int = 0
    ) -> List[Dict[str, Any]]:
        """Get all facts for an investigation with pagination."""
        async with self._lock:
            inv = self._storage.get(investigation_id, {})
            facts = list(inv.get("facts", {}).values())

            # Sort by extraction time (newest first)
            facts.sort(
                key=lambda f: f.get("extraction", {}).get("extracted_at", ""),
                reverse=True
            )

            if limit:
                return facts[offset:offset + limit]
            return facts[offset:]

    async def link_variants(
        self,
        investigation_id: str,
        primary_fact_id: str,
        variant_fact_ids: List[str]
    ) -> bool:
        """
        Link facts as semantic variants of a primary fact.

        Args:
            investigation_id: Investigation identifier
            primary_fact_id: ID of the canonical fact
            variant_fact_ids: IDs of semantic duplicates to link

        Returns:
            True if linking succeeded
        """
        async with self._lock:
            inv = self._storage.get(investigation_id, {})
            primary = inv.get("facts", {}).get(primary_fact_id)

            if not primary:
                return False

            variants = primary.get("variants", [])
            for vid in variant_fact_ids:
                if vid not in variants and vid != primary_fact_id:
                    variants.append(vid)

            primary["variants"] = variants
            inv["updated_at"] = datetime.now(timezone.utc).isoformat()

            if self.persistence_path:
                self._save_to_file()

            return True

    async def get_stats(self, investigation_id: str) -> Dict[str, Any]:
        """Get statistics for an investigation."""
        async with self._lock:
            inv = self._storage.get(investigation_id, {})
            if not inv:
                return {"exists": False, "investigation_id": investigation_id}

            facts = inv.get("facts", {})

            # Count by source
            source_counts = {}
            for fact in facts.values():
                source_id = fact.get("provenance", {}).get("source_id", "unknown")
                source_counts[source_id] = source_counts.get(source_id, 0) + 1

            # Count by assertion type
            assertion_counts = {}
            for fact in facts.values():
                assertion = fact.get("claim", {}).get("assertion_type", "unknown")
                assertion_counts[assertion] = assertion_counts.get(assertion, 0) + 1

            return {
                "exists": True,
                "investigation_id": investigation_id,
                "total_facts": len(facts),
                "unique_hashes": len(inv.get("hash_index", {})),
                "sources": len(inv.get("source_index", {})),
                "source_breakdown": source_counts,
                "assertion_breakdown": assertion_counts,
                "created_at": inv.get("created_at"),
                "updated_at": inv.get("updated_at")
            }

    async def delete_investigation(self, investigation_id: str) -> bool:
        """Delete an investigation and all its facts."""
        async with self._lock:
            if investigation_id not in self._storage:
                return False

            del self._storage[investigation_id]

            if self.persistence_path:
                self._save_to_file()

            self.logger.info(f"Deleted investigation: {investigation_id}")
            return True

    def _save_to_file(self) -> None:
        """Save to JSON file (synchronous)."""
        if not self.persistence_path:
            return
        try:
            self.persistence_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.persistence_path, 'w') as f:
                json.dump(self._storage, f, indent=2, default=str)
        except Exception as e:
            self.logger.error(f"Failed to persist: {e}")

    def _load_from_file(self) -> None:
        """Load from JSON file (synchronous)."""
        if not self.persistence_path or not self.persistence_path.exists():
            return
        try:
            with open(self.persistence_path, 'r') as f:
                self._storage = json.load(f)
            self.logger.info(f"Loaded from {self.persistence_path}")
        except Exception as e:
            self.logger.error(f"Failed to load: {e}")
            self._storage = {}
```

Update osint_system/data_management/__init__.py to export FactStore.
  </action>
  <verify>
```bash
uv run python -c "
import asyncio
from osint_system.data_management.fact_store import FactStore

async def test():
    store = FactStore()
    facts = [
        {'fact_id': 'f1', 'content_hash': 'abc123', 'claim': {'text': 'Test'}},
        {'fact_id': 'f2', 'content_hash': 'abc123', 'claim': {'text': 'Test'}},  # dup hash
    ]
    stats = await store.save_facts('inv-1', facts)
    print(f'Stats: {stats}')
    assert stats['saved'] == 2, 'Both saved (different IDs)'
    assert stats['updated'] == 1, 'One updated as variant'

    fact = await store.get_fact('inv-1', 'f1')
    print(f'Variants: {fact.get(\"variants\", [])}')
    print('FactStore OK')

asyncio.run(test())
"
```
  </verify>
  <done>FactStore provides O(1) lookup by fact_id and content_hash with investigation scoping</done>
</task>

<task type="auto">
  <name>Task 2: Implement FactConsolidator</name>
  <files>osint_system/agents/sifters/fact_consolidator.py</files>
  <action>
Create consolidator for semantic deduplication and variant linking.

**fact_consolidator.py:**
```python
"""Fact consolidation with semantic deduplication and variant linking."""

import hashlib
from typing import List, Dict, Any, Optional, Tuple
from loguru import logger

from osint_system.agents.sifters.base_sifter import BaseSifter
from osint_system.data_management.fact_store import FactStore
from osint_system.data_management.schemas import ExtractedFact


class FactConsolidator(BaseSifter):
    """
    Consolidates extracted facts with multi-layer deduplication.

    Deduplication layers (per CONTEXT.md):
    1. Exact match: content_hash collision = same fact
    2. Semantic similarity: embedding cosine > threshold = variants
    3. Noise filter: similarity < 0.3 = discard

    Consolidation preserves corroboration:
    - Multiple sources for same claim -> one canonical + variants list
    - Each variant retains its provenance
    - Source count is intelligence signal

    Attributes:
        fact_store: FactStore for persistence
        similarity_threshold: Above this, facts are linked as variants (default 0.3)
        use_embeddings: Whether to use embedding similarity (requires model)
    """

    # Per CONTEXT.md: 0.3 is the dedup threshold (permissive)
    DEFAULT_SIMILARITY_THRESHOLD = 0.3

    def __init__(
        self,
        fact_store: Optional[FactStore] = None,
        similarity_threshold: float = DEFAULT_SIMILARITY_THRESHOLD,
        use_embeddings: bool = False,
        embedding_model: Optional[Any] = None,
    ):
        super().__init__(
            name="FactConsolidator",
            description="Consolidates facts with deduplication and variant linking"
        )
        self.fact_store = fact_store or FactStore()
        self.similarity_threshold = similarity_threshold
        self.use_embeddings = use_embeddings
        self.embedding_model = embedding_model
        self._embedding_cache: Dict[str, List[float]] = {}

        self.logger.info(
            "FactConsolidator initialized",
            threshold=similarity_threshold,
            embeddings=use_embeddings
        )

    async def sift(self, content: dict) -> list[dict]:
        """
        Consolidate facts from content.

        Args:
            content: {
                'facts': list of ExtractedFact dicts,
                'investigation_id': str
            }

        Returns:
            Consolidated facts (canonical + variants linked)
        """
        facts = content.get("facts", [])
        investigation_id = content.get("investigation_id", "default")

        if not facts:
            return []

        # Layer 1: Exact hash dedup
        unique_facts, hash_groups = self._dedupe_by_hash(facts)

        # Layer 2: Semantic similarity (if enabled)
        if self.use_embeddings and len(unique_facts) > 1:
            unique_facts = await self._dedupe_by_similarity(unique_facts)

        # Link variants within hash groups
        for hash_val, group in hash_groups.items():
            if len(group) > 1:
                primary = group[0]
                variants = [f["fact_id"] for f in group[1:]]
                primary["variants"] = primary.get("variants", []) + variants

        # Store consolidated facts
        if investigation_id:
            await self.fact_store.save_facts(investigation_id, unique_facts)

        self.logger.info(
            f"Consolidated {len(facts)} -> {len(unique_facts)} facts",
            investigation_id=investigation_id
        )

        return unique_facts

    def _dedupe_by_hash(
        self,
        facts: List[Dict[str, Any]]
    ) -> Tuple[List[Dict[str, Any]], Dict[str, List[Dict[str, Any]]]]:
        """
        Layer 1: Exact content hash deduplication.

        Returns:
            (unique_facts, hash_groups) - groups for variant linking
        """
        hash_groups: Dict[str, List[Dict[str, Any]]] = {}
        unique_facts = []

        for fact in facts:
            content_hash = fact.get("content_hash")

            if not content_hash:
                # Compute hash if missing
                claim_text = fact.get("claim", {}).get("text", "")
                content_hash = hashlib.sha256(claim_text.encode()).hexdigest()
                fact["content_hash"] = content_hash

            if content_hash not in hash_groups:
                hash_groups[content_hash] = []
                unique_facts.append(fact)

            hash_groups[content_hash].append(fact)

        self.logger.debug(
            f"Hash dedup: {len(facts)} -> {len(unique_facts)} unique hashes"
        )

        return unique_facts, hash_groups

    async def _dedupe_by_similarity(
        self,
        facts: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Layer 2: Semantic similarity deduplication.

        Facts with similarity > threshold become variants.
        Facts with similarity < threshold are discarded as noise.
        """
        if not self.embedding_model or len(facts) <= 1:
            return facts

        # Get embeddings for all claim texts
        embeddings = []
        for fact in facts:
            claim_text = fact.get("claim", {}).get("text", "")
            embedding = await self._get_embedding(claim_text)
            embeddings.append(embedding)

        # Find similar pairs and cluster
        canonical_indices = set(range(len(facts)))
        variant_map: Dict[int, List[int]] = {}  # canonical_idx -> [variant_idx]

        for i in range(len(facts)):
            if i not in canonical_indices:
                continue

            for j in range(i + 1, len(facts)):
                if j not in canonical_indices:
                    continue

                similarity = self._cosine_similarity(embeddings[i], embeddings[j])

                if similarity >= self.similarity_threshold:
                    # Link as variant
                    if i not in variant_map:
                        variant_map[i] = []
                    variant_map[i].append(j)
                    canonical_indices.discard(j)

                elif similarity < self.similarity_threshold:
                    # Below threshold - could discard, but we keep for now
                    # (discarding happens at extraction time via min_confidence)
                    pass

        # Build result with variant links
        result = []
        for i in canonical_indices:
            fact = facts[i]
            variants = variant_map.get(i, [])
            if variants:
                fact["variants"] = fact.get("variants", []) + [
                    facts[v]["fact_id"] for v in variants
                ]
            result.append(fact)

        return result

    async def _get_embedding(self, text: str) -> List[float]:
        """Get embedding for text, with caching."""
        cache_key = hashlib.md5(text.encode()).hexdigest()

        if cache_key in self._embedding_cache:
            return self._embedding_cache[cache_key]

        if not self.embedding_model:
            # Return empty if no model - similarity will be 0
            return []

        try:
            # Assumes embedding_model has embed_content method (Gemini pattern)
            result = self.embedding_model.embed_content(text)
            embedding = result.get("embedding", [])
            self._embedding_cache[cache_key] = embedding
            return embedding
        except Exception as e:
            self.logger.warning(f"Embedding failed: {e}")
            return []

    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """Compute cosine similarity between two vectors."""
        if not a or not b or len(a) != len(b):
            return 0.0

        dot_product = sum(x * y for x, y in zip(a, b))
        norm_a = sum(x * x for x in a) ** 0.5
        norm_b = sum(y * y for y in b) ** 0.5

        if norm_a == 0 or norm_b == 0:
            return 0.0

        return dot_product / (norm_a * norm_b)

    async def consolidate_batch(
        self,
        investigation_id: str,
        fact_batches: List[List[Dict[str, Any]]]
    ) -> Dict[str, Any]:
        """
        Consolidate multiple batches of facts.

        Useful when processing facts from multiple sources/crawlers.

        Args:
            investigation_id: Investigation identifier
            fact_batches: List of fact lists from different sources

        Returns:
            Consolidation stats
        """
        all_facts = []
        for batch in fact_batches:
            all_facts.extend(batch)

        result = await self.sift({
            "facts": all_facts,
            "investigation_id": investigation_id
        })

        return {
            "input_count": len(all_facts),
            "output_count": len(result),
            "reduction": len(all_facts) - len(result),
            "investigation_id": investigation_id
        }

    def get_capabilities(self) -> list[str]:
        return [
            "fact_consolidation",
            "deduplication",
            "variant_linking",
            "semantic_similarity"
        ]
```

Update osint_system/agents/sifters/__init__.py to export FactConsolidator.
  </action>
  <verify>
```bash
uv run python -c "
import asyncio
from osint_system.agents.sifters.fact_consolidator import FactConsolidator

async def test():
    consolidator = FactConsolidator()
    facts = [
        {'fact_id': 'f1', 'claim': {'text': 'Putin visited Beijing'}},
        {'fact_id': 'f2', 'claim': {'text': 'Putin visited Beijing'}},  # exact dup
        {'fact_id': 'f3', 'claim': {'text': 'Different claim'}},
    ]
    result = await consolidator.sift({
        'facts': facts,
        'investigation_id': 'test-inv'
    })
    print(f'Input: {len(facts)}, Output: {len(result)}')
    # f1 and f2 have same text -> same hash -> f1 is canonical, f2 is variant
    assert len(result) == 2, 'Should dedupe identical claims'
    print('Consolidator OK')

asyncio.run(test())
"
```
  </verify>
  <done>FactConsolidator deduplicates by content hash and links variants</done>
</task>

<task type="auto">
  <name>Task 3: Add storage and consolidation tests</name>
  <files>
    tests/data_management/test_fact_store.py
    tests/agents/sifters/test_fact_consolidator.py
  </files>
  <action>
Create comprehensive tests for FactStore and FactConsolidator.

**test_fact_store.py:**
Test cases:
1. Save and retrieve single fact
2. O(1) lookup by fact_id
3. O(1) lookup by content_hash
4. Duplicate hash handling (variant linking)
5. Source indexing
6. Investigation scoping (same fact in different investigations)
7. Statistics calculation
8. Persistence (save/load cycle)
9. Delete investigation

**test_fact_consolidator.py:**
Test cases:
1. Hash deduplication (exact match)
2. Variant linking for same-hash facts
3. Different hashes remain separate
4. Empty input handling
5. Batch consolidation
6. Store integration

Use pytest and follow existing test patterns.
Create tests/data_management/__init__.py if needed.
  </action>
  <verify>
```bash
uv run python -m pytest tests/data_management/test_fact_store.py tests/agents/sifters/test_fact_consolidator.py -v
```
  </verify>
  <done>All storage and consolidation tests pass</done>
</task>

</tasks>

<verification>
```bash
# All imports work
uv run python -c "
from osint_system.data_management.fact_store import FactStore
from osint_system.agents.sifters.fact_consolidator import FactConsolidator
print('All imports OK')
"

# All tests pass
uv run python -m pytest tests/data_management/test_fact_store.py tests/agents/sifters/test_fact_consolidator.py -v

# Integration check
uv run python -c "
import asyncio
from osint_system.data_management.fact_store import FactStore
from osint_system.agents.sifters.fact_consolidator import FactConsolidator

async def integration():
    store = FactStore()
    consolidator = FactConsolidator(fact_store=store)

    facts = [
        {'fact_id': 'f1', 'claim': {'text': 'Test claim 1'}, 'provenance': {'source_id': 's1'}},
        {'fact_id': 'f2', 'claim': {'text': 'Test claim 1'}, 'provenance': {'source_id': 's2'}},
        {'fact_id': 'f3', 'claim': {'text': 'Test claim 2'}, 'provenance': {'source_id': 's1'}},
    ]

    result = await consolidator.sift({'facts': facts, 'investigation_id': 'int-test'})
    stats = await store.get_stats('int-test')

    print(f'Consolidated: {len(facts)} -> {len(result)}')
    print(f'Store stats: {stats}')
    assert stats['total_facts'] == 2, 'Two unique claims'
    print('Integration OK')

asyncio.run(integration())
"
```
</verification>

<success_criteria>
- FactStore provides O(1) lookup by fact_id and content_hash
- Investigation scoping prevents cross-investigation pollution
- Exact duplicates (same hash) are detected and variant-linked
- FactConsolidator applies multi-layer deduplication
- Semantic similarity available when embeddings enabled
- Below 0.3 threshold facts can be discarded (configurable)
- Multiple sources for same claim preserved with provenance
- All tests pass
- Integration between consolidator and store works
</success_criteria>

<output>
After completion, create `.planning/phases/06-fact-extraction-pipeline/06-03-SUMMARY.md`
</output>
