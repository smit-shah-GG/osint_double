---
phase: 06-fact-extraction-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - osint_system/data_management/schemas/__init__.py
  - osint_system/data_management/schemas/fact_schema.py
  - osint_system/data_management/schemas/entity_schema.py
  - osint_system/data_management/schemas/provenance_schema.py
  - tests/data_management/schemas/test_fact_schema.py
autonomous: true

must_haves:
  truths:
    - "Fact records validate with required fields (fact_id, content_hash, claim.text)"
    - "Optional fields don't cause validation failure when missing"
    - "Schema version is explicit and tracked"
    - "Entity markers in claim text link to entity objects by ID"
    - "Provenance chains capture attribution depth"
  artifacts:
    - path: "osint_system/data_management/schemas/fact_schema.py"
      provides: "ExtractedFact Pydantic model with full schema"
      exports: ["ExtractedFact", "Claim", "TemporalMarker", "QualityMetrics", "ExtractionMetadata"]
      min_lines: 150
    - path: "osint_system/data_management/schemas/entity_schema.py"
      provides: "Entity models for PERSON, ORG, LOCATION, anonymous sources"
      exports: ["Entity", "AnonymousSource", "EntityCluster"]
      min_lines: 80
    - path: "osint_system/data_management/schemas/provenance_schema.py"
      provides: "Provenance chain and source attribution models"
      exports: ["Provenance", "AttributionHop", "SourceType", "SourceClassification"]
      min_lines: 60
  key_links:
    - from: "osint_system/data_management/schemas/fact_schema.py"
      to: "osint_system/data_management/schemas/entity_schema.py"
      via: "Entity import for entities field"
      pattern: "from.*entity_schema.*import.*Entity"
    - from: "osint_system/data_management/schemas/fact_schema.py"
      to: "osint_system/data_management/schemas/provenance_schema.py"
      via: "Provenance import for provenance field"
      pattern: "from.*provenance_schema.*import.*Provenance"
---

<objective>
Define comprehensive Pydantic schemas for the fact extraction output format per CONTEXT.md decisions.

Purpose: Typed data structures enable validated LLM output, IDE autocomplete, and downstream phase compatibility. Schema decisions from discuss-phase are encoded as code, not documentation.

Output: Complete schema package with ExtractedFact as primary export, ready for FactExtractionAgent consumption.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-fact-extraction-pipeline/06-CONTEXT.md

# Existing patterns
@osint_system/data_management/article_store.py
@osint_system/orchestration/state_schemas.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create entity and provenance schemas</name>
  <files>
    osint_system/data_management/schemas/__init__.py
    osint_system/data_management/schemas/entity_schema.py
    osint_system/data_management/schemas/provenance_schema.py
  </files>
  <action>
Create the schemas/ directory and implement entity and provenance models.

**entity_schema.py:**
```python
from pydantic import BaseModel, Field
from typing import Optional, Literal
from enum import Enum

class EntityType(str, Enum):
    PERSON = "PERSON"
    ORGANIZATION = "ORGANIZATION"
    LOCATION = "LOCATION"
    EVENT = "EVENT"
    DATE = "DATE"
    ANONYMOUS_SOURCE = "ANONYMOUS_SOURCE"

class Entity(BaseModel):
    id: str = Field(..., description="Entity ID (E1, E2, etc) for linking to claim text")
    text: str = Field(..., description="Original text span")
    type: EntityType
    canonical: Optional[str] = Field(None, description="Normalized form (e.g., 'Vladimir Putin')")
    cluster_id: Optional[str] = Field(None, description="ID for entity clustering without forced resolution")

class AnonymousSource(BaseModel):
    """Structured representation of anonymous sources with available metadata."""
    entity_type: Literal["anonymous_source"] = "anonymous_source"
    descriptors: dict = Field(default_factory=dict)  # role, affiliation, department, seniority
    anonymity_granted_by: Optional[str] = Field(None, description="Source document ID")

class EntityCluster(BaseModel):
    """Group of likely-same entities without forced resolution."""
    cluster_id: str
    entities: list[str] = Field(default_factory=list, description="Entity IDs in cluster")
    canonical_suggestion: Optional[str] = None
```

**provenance_schema.py:**
```python
from pydantic import BaseModel, Field
from typing import Optional, Literal
from enum import Enum

class SourceType(str, Enum):
    WIRE_SERVICE = "wire_service"
    OFFICIAL_STATEMENT = "official_statement"
    NEWS_OUTLET = "news_outlet"
    SOCIAL_MEDIA = "social_media"
    ACADEMIC = "academic"
    DOCUMENT = "document"
    EYEWITNESS = "eyewitness"
    UNKNOWN = "unknown"

class SourceClassification(str, Enum):
    PRIMARY = "primary"      # Direct observation/statement
    SECONDARY = "secondary"  # Journalistic reporting
    TERTIARY = "tertiary"    # Aggregation/analysis

class AttributionHop(BaseModel):
    """Single hop in attribution chain."""
    entity: str = Field(..., description="Who is being cited")
    type: SourceType
    hop: int = Field(..., ge=0, description="Distance from original (0 = eyewitness)")

class Provenance(BaseModel):
    """Full provenance tracking for a fact."""
    source_id: str = Field(..., description="ID of source document")
    quote: str = Field(..., description="Exact quoted text span")
    offsets: dict = Field(..., description="{'start': int, 'end': int} character positions")
    attribution_chain: list[AttributionHop] = Field(default_factory=list)
    attribution_phrase: Optional[str] = Field(None, description="Original attribution phrasing")
    hop_count: int = Field(0, ge=0)
    source_type: SourceType = SourceType.UNKNOWN
    source_classification: SourceClassification = SourceClassification.SECONDARY
```

**__init__.py:** Lazy imports for all exports.
  </action>
  <verify>
`uv run python -c "from osint_system.data_management.schemas import Entity, Provenance, SourceType; print('Imports OK')"`
  </verify>
  <done>Entity and provenance schemas importable with all fields from CONTEXT.md</done>
</task>

<task type="auto">
  <name>Task 2: Create core fact schema</name>
  <files>osint_system/data_management/schemas/fact_schema.py</files>
  <action>
Implement the main ExtractedFact schema per CONTEXT.md example.

**fact_schema.py:**
```python
import uuid
import hashlib
from datetime import datetime
from pydantic import BaseModel, Field, field_validator, model_validator
from typing import Optional, Literal, Any

from .entity_schema import Entity, EntityCluster
from .provenance_schema import Provenance

SCHEMA_VERSION = "1.0"

class Claim(BaseModel):
    """The assertion being made."""
    text: str = Field(..., description="Claim with entity markers like [E1:Putin]")
    assertion_type: Literal["statement", "denial", "claim", "prediction", "quote"] = "statement"
    claim_type: Literal["event", "state", "relationship", "prediction", "planned"] = "event"

class TemporalMarker(BaseModel):
    """Temporal information with precision tracking."""
    id: str = Field(..., description="Temporal ID (T1, T2, etc)")
    value: str = Field(..., description="ISO-ish value: 2024-03, 2024-03-15, etc")
    precision: Literal["year", "month", "day", "time", "range"] = "day"
    temporal_precision: Literal["explicit", "inferred", "unknown"] = Field(
        ..., description="explicit=stated in text, inferred=from article date, unknown=unclear"
    )

class NumericValue(BaseModel):
    """Numerical claims with precision preservation."""
    value_original: str = Field(..., description="Original text: 'thousands', '~50'")
    value_normalized: Optional[list] = Field(None, description="[min, max] range if applicable")
    numeric_precision: Literal["exact", "approximate", "order_of_magnitude"] = "exact"

class ExtractionTrace(BaseModel):
    """Detailed reasoning for debugging/audit."""
    parsing_notes: Optional[str] = None
    clarity_factors: list[str] = Field(default_factory=list)
    entity_resolution: Optional[str] = None

class QualityMetrics(BaseModel):
    """Separate dimensions per CONTEXT.md: extraction_confidence vs claim_clarity."""
    extraction_confidence: float = Field(..., ge=0.0, le=1.0, description="LLM parsing accuracy")
    claim_clarity: float = Field(..., ge=0.0, le=1.0, description="Source text ambiguity")
    extraction_trace: Optional[ExtractionTrace] = None

class ExtractionMetadata(BaseModel):
    """Metadata about extraction process."""
    extracted_at: datetime = Field(default_factory=datetime.utcnow)
    model_version: str = "gemini-1.5-flash"
    extraction_type: Literal["explicit", "inferred"] = "explicit"

class FactRelationship(BaseModel):
    """Relationship hints to other facts."""
    type: Literal["supports", "contradicts", "temporal_sequence", "elaborates"]
    target_fact_id: str
    confidence: float = Field(0.5, ge=0.0, le=1.0)

class ExtractedFact(BaseModel):
    """
    Complete fact record per Phase 6 CONTEXT.md schema.

    Only hard requirements: fact_id, claim.text
    Everything else optional but captured when available.
    """
    schema_version: str = SCHEMA_VERSION
    fact_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    content_hash: str = Field("", description="SHA256 of claim.text for dedup")

    # Core content
    claim: Claim
    entities: list[Entity] = Field(default_factory=list)
    temporal: Optional[TemporalMarker] = None
    numeric: Optional[NumericValue] = None

    # Attribution
    provenance: Optional[Provenance] = None

    # Quality
    quality: Optional[QualityMetrics] = None

    # Process metadata
    extraction: ExtractionMetadata = Field(default_factory=ExtractionMetadata)

    # Relationships
    relationships: list[FactRelationship] = Field(default_factory=list)
    variants: list[str] = Field(default_factory=list, description="IDs of semantic duplicates")

    @model_validator(mode='after')
    def compute_content_hash(self) -> 'ExtractedFact':
        """Compute content hash from claim text if not provided."""
        if not self.content_hash:
            self.content_hash = hashlib.sha256(
                self.claim.text.encode('utf-8')
            ).hexdigest()
        return self

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "schema_version": "1.0",
                    "fact_id": "uuid-here",
                    "claim": {
                        "text": "[E1:Putin] visited [E2:Beijing] in [T1:March 2024]",
                        "assertion_type": "statement",
                        "claim_type": "event"
                    },
                    "entities": [
                        {"id": "E1", "text": "Putin", "type": "PERSON", "canonical": "Vladimir Putin"}
                    ]
                }
            ]
        }
    }
```

Update __init__.py to export fact_schema types.
  </action>
  <verify>
```bash
uv run python -c "
from osint_system.data_management.schemas import ExtractedFact, Claim
fact = ExtractedFact(claim=Claim(text='Test claim'))
print(f'fact_id: {fact.fact_id[:8]}...')
print(f'content_hash: {fact.content_hash[:16]}...')
print(f'schema_version: {fact.schema_version}')
assert fact.content_hash, 'Hash should be computed'
print('Validation OK')
"
```
  </verify>
  <done>ExtractedFact creates valid facts with auto-computed content hash and UUID</done>
</task>

<task type="auto">
  <name>Task 3: Add schema tests</name>
  <files>tests/data_management/schemas/test_fact_schema.py</files>
  <action>
Create test file with comprehensive schema validation tests.

**Tests to include:**
1. Minimal valid fact (only claim.text required)
2. Full fact with all optional fields
3. Content hash auto-computation
4. Entity linking validation
5. Provenance chain serialization
6. Quality metrics separate dimensions
7. Schema version in output
8. Denial assertion type handling
9. Anonymous source entity

Create tests/data_management/schemas/__init__.py (empty).

Use pytest patterns from existing tests (check tests/ structure).
  </action>
  <verify>
`uv run python -m pytest tests/data_management/schemas/test_fact_schema.py -v`
  </verify>
  <done>All schema validation tests pass</done>
</task>

</tasks>

<verification>
```bash
# All imports work
uv run python -c "
from osint_system.data_management.schemas import (
    ExtractedFact, Claim, Entity, Provenance,
    SourceType, SourceClassification, QualityMetrics,
    EntityType, AttributionHop, TemporalMarker
)
print('All schema imports OK')
"

# Tests pass
uv run python -m pytest tests/data_management/schemas/ -v
```
</verification>

<success_criteria>
- All Pydantic models validate correctly
- ExtractedFact accepts minimal input (just claim text)
- ExtractedFact accepts full input (all fields)
- Content hash auto-computed from claim text
- Schema version explicit in every fact
- Entity markers link to entity objects by ID pattern
- Provenance captures full attribution chain
- Quality has separate extraction_confidence and claim_clarity
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/06-fact-extraction-pipeline/06-01-SUMMARY.md`
</output>
