---
phase: 06-fact-extraction-pipeline
plan: 04
type: execute
wave: 4
depends_on: ["06-03"]
files_modified:
  - osint_system/pipelines/__init__.py
  - osint_system/pipelines/extraction_pipeline.py
  - tests/pipelines/test_extraction_pipeline.py
autonomous: true

must_haves:
  truths:
    - "Articles from ArticleStore flow to FactExtractionAgent automatically"
    - "Extracted facts flow to FactConsolidator automatically"
    - "Consolidated facts are stored in FactStore"
    - "Pipeline can process an investigation's articles end-to-end"
  artifacts:
    - path: "osint_system/pipelines/extraction_pipeline.py"
      provides: "ExtractionPipeline orchestrating article-to-fact flow"
      exports: ["ExtractionPipeline"]
      min_lines: 100
  key_links:
    - from: "osint_system/pipelines/extraction_pipeline.py"
      to: "osint_system/data_management/article_store.py"
      via: "ArticleStore for reading crawler output"
      pattern: "from.*article_store.*import.*ArticleStore"
    - from: "osint_system/pipelines/extraction_pipeline.py"
      to: "osint_system/agents/sifters/fact_extraction_agent.py"
      via: "FactExtractionAgent for LLM extraction"
      pattern: "from.*fact_extraction_agent.*import.*FactExtractionAgent"
    - from: "osint_system/pipelines/extraction_pipeline.py"
      to: "osint_system/agents/sifters/fact_consolidator.py"
      via: "FactConsolidator for dedup and storage"
      pattern: "from.*fact_consolidator.*import.*FactConsolidator"
---

<objective>
Create pipeline that bridges crawler output (ArticleStore) to fact extraction and consolidation.

Purpose: Plans 01-03 created the extraction components (schemas, agent, consolidator, store). This plan wires them together so an investigation's crawled articles automatically flow through extraction and end up as consolidated facts in FactStore.

Output: ExtractionPipeline class that orchestrates the full article-to-fact flow for an investigation.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-fact-extraction-pipeline/06-CONTEXT.md
@.planning/phases/06-fact-extraction-pipeline/06-01-SUMMARY.md
@.planning/phases/06-fact-extraction-pipeline/06-02-SUMMARY.md
@.planning/phases/06-fact-extraction-pipeline/06-03-SUMMARY.md

# Components to integrate
@osint_system/data_management/article_store.py
@osint_system/agents/sifters/fact_extraction_agent.py
@osint_system/agents/sifters/fact_consolidator.py
@osint_system/data_management/fact_store.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ExtractionPipeline</name>
  <files>
    osint_system/pipelines/__init__.py
    osint_system/pipelines/extraction_pipeline.py
  </files>
  <action>
Create the pipelines directory and implement ExtractionPipeline.

**extraction_pipeline.py:**
```python
"""Pipeline orchestrating article-to-fact extraction flow.

Bridges crawler output (ArticleStore) to extraction pipeline:
1. Read articles from ArticleStore
2. Pass each article to FactExtractionAgent
3. Consolidate extracted facts via FactConsolidator
4. Store final facts in FactStore
"""

import asyncio
from typing import Optional, Dict, Any, List
from datetime import datetime, timezone
from loguru import logger

from osint_system.data_management.article_store import ArticleStore
from osint_system.data_management.fact_store import FactStore
from osint_system.agents.sifters.fact_extraction_agent import FactExtractionAgent
from osint_system.agents.sifters.fact_consolidator import FactConsolidator


class ExtractionPipeline:
    """
    Orchestrates the full article-to-fact extraction flow.

    This pipeline bridges the gap between:
    - Crawler output (articles in ArticleStore)
    - Extraction pipeline input (FactExtractionAgent)
    - Storage (consolidated facts in FactStore)

    Usage:
        pipeline = ExtractionPipeline()
        result = await pipeline.process_investigation("inv-123")

    Attributes:
        article_store: Source of crawled articles
        fact_store: Destination for extracted facts
        extraction_agent: LLM-powered fact extractor
        consolidator: Deduplication and variant linking
        batch_size: Articles to process before consolidation
    """

    DEFAULT_BATCH_SIZE = 10

    def __init__(
        self,
        article_store: Optional[ArticleStore] = None,
        fact_store: Optional[FactStore] = None,
        extraction_agent: Optional[FactExtractionAgent] = None,
        consolidator: Optional[FactConsolidator] = None,
        batch_size: int = DEFAULT_BATCH_SIZE,
    ):
        """
        Initialize extraction pipeline.

        Args:
            article_store: Source of articles (creates default if None)
            fact_store: Destination for facts (creates default if None)
            extraction_agent: Fact extractor (creates default if None)
            consolidator: Dedup engine (creates default if None)
            batch_size: Articles per batch before consolidation
        """
        self.article_store = article_store or ArticleStore()
        self.fact_store = fact_store or FactStore()
        self.extraction_agent = extraction_agent or FactExtractionAgent()
        self.consolidator = consolidator or FactConsolidator(fact_store=self.fact_store)
        self.batch_size = batch_size
        self.logger = logger.bind(component="ExtractionPipeline")

        self.logger.info(
            "ExtractionPipeline initialized",
            batch_size=batch_size
        )

    async def process_investigation(
        self,
        investigation_id: str,
        since: Optional[str] = None,
        limit: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        Process all articles for an investigation through extraction pipeline.

        Args:
            investigation_id: Investigation identifier
            since: Only process articles stored after this ISO timestamp
            limit: Maximum articles to process (None = all)

        Returns:
            Processing statistics:
            - articles_processed: Number of articles processed
            - facts_extracted: Total facts from extraction
            - facts_consolidated: Facts after deduplication
            - processing_time_seconds: Total time
            - errors: List of any errors encountered
        """
        start_time = datetime.now(timezone.utc)
        self.logger.info(f"Starting extraction for investigation: {investigation_id}")

        stats = {
            "investigation_id": investigation_id,
            "articles_processed": 0,
            "facts_extracted": 0,
            "facts_consolidated": 0,
            "errors": [],
            "started_at": start_time.isoformat()
        }

        try:
            # Step 1: Retrieve articles from ArticleStore
            if since:
                articles = await self.article_store.retrieve_recent_articles(
                    investigation_id, since=since, limit=limit
                )
            else:
                result = await self.article_store.retrieve_by_investigation(
                    investigation_id, limit=limit
                )
                articles = result.get("articles", [])

            if not articles:
                self.logger.warning(f"No articles found for {investigation_id}")
                stats["processing_time_seconds"] = 0
                return stats

            self.logger.info(f"Found {len(articles)} articles to process")

            # Step 2: Extract facts in batches
            all_facts = []
            for i in range(0, len(articles), self.batch_size):
                batch = articles[i:i + self.batch_size]
                batch_facts = await self._process_batch(batch, stats)
                all_facts.extend(batch_facts)
                stats["articles_processed"] += len(batch)

            stats["facts_extracted"] = len(all_facts)

            # Step 3: Consolidate all facts
            if all_facts:
                consolidated = await self.consolidator.sift({
                    "facts": all_facts,
                    "investigation_id": investigation_id
                })
                stats["facts_consolidated"] = len(consolidated)
            else:
                stats["facts_consolidated"] = 0

            end_time = datetime.now(timezone.utc)
            stats["processing_time_seconds"] = (end_time - start_time).total_seconds()
            stats["completed_at"] = end_time.isoformat()

            self.logger.info(
                f"Extraction complete for {investigation_id}",
                articles=stats["articles_processed"],
                facts_extracted=stats["facts_extracted"],
                facts_consolidated=stats["facts_consolidated"],
                time=stats["processing_time_seconds"]
            )

            return stats

        except Exception as e:
            self.logger.error(f"Pipeline failed: {e}", exc_info=True)
            stats["errors"].append(str(e))
            end_time = datetime.now(timezone.utc)
            stats["processing_time_seconds"] = (end_time - start_time).total_seconds()
            return stats

    async def _process_batch(
        self,
        articles: List[Dict[str, Any]],
        stats: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Process a batch of articles through extraction.

        Args:
            articles: List of article dicts from ArticleStore
            stats: Stats dict to update with errors

        Returns:
            List of extracted fact dicts
        """
        batch_facts = []

        for article in articles:
            try:
                # Transform article to extraction input format
                content = self._article_to_content(article)

                # Extract facts
                facts = await self.extraction_agent.sift(content)
                batch_facts.extend(facts)

                self.logger.debug(
                    f"Extracted {len(facts)} facts from article",
                    url=article.get("url", "unknown")[:50]
                )

            except Exception as e:
                error_msg = f"Failed to process article {article.get('url', 'unknown')}: {e}"
                self.logger.warning(error_msg)
                stats["errors"].append(error_msg)
                continue

        return batch_facts

    def _article_to_content(self, article: Dict[str, Any]) -> Dict[str, Any]:
        """
        Transform ArticleStore article format to FactExtractionAgent content format.

        ArticleStore format:
        {
            "url": "...",
            "title": "...",
            "content": "...",
            "published_date": "...",
            "source": {"name": "...", ...},
            "metadata": {...}
        }

        FactExtractionAgent format:
        {
            "text": "...",
            "source_id": "...",
            "source_type": "...",
            "publication_date": "...",
            "metadata": {...}
        }
        """
        # Combine title and content for full text
        title = article.get("title", "")
        content = article.get("content", "")
        text = f"{title}\n\n{content}" if title else content

        # Derive source_id from URL (most stable identifier)
        source_id = article.get("url", article.get("stored_at", "unknown"))

        # Derive source_type from source metadata
        source = article.get("source", {})
        source_type = source.get("type", "news_outlet")

        return {
            "text": text,
            "source_id": source_id,
            "source_type": source_type,
            "publication_date": article.get("published_date", ""),
            "metadata": {
                "original_url": article.get("url"),
                "source_name": source.get("name"),
                "stored_at": article.get("stored_at"),
                **article.get("metadata", {})
            }
        }

    async def process_single_article(
        self,
        investigation_id: str,
        article: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Process a single article through extraction (for real-time processing).

        Args:
            investigation_id: Investigation identifier
            article: Article dict in ArticleStore format

        Returns:
            Processing result with facts
        """
        content = self._article_to_content(article)
        facts = await self.extraction_agent.sift(content)

        if facts:
            consolidated = await self.consolidator.sift({
                "facts": facts,
                "investigation_id": investigation_id
            })
        else:
            consolidated = []

        return {
            "facts_extracted": len(facts),
            "facts_consolidated": len(consolidated),
            "facts": consolidated
        }

    async def get_pipeline_status(self, investigation_id: str) -> Dict[str, Any]:
        """
        Get status of pipeline for an investigation.

        Returns article and fact counts for monitoring progress.
        """
        article_stats = await self.article_store.get_investigation_stats(investigation_id)
        fact_stats = await self.fact_store.get_stats(investigation_id)

        return {
            "investigation_id": investigation_id,
            "articles": {
                "total": article_stats.get("total_articles", 0),
                "sources": article_stats.get("source_breakdown", {})
            },
            "facts": {
                "total": fact_stats.get("total_facts", 0),
                "unique_claims": fact_stats.get("unique_hashes", 0),
                "sources": fact_stats.get("sources", 0)
            }
        }
```

**__init__.py:**
```python
"""Pipelines for orchestrating multi-component workflows."""

from osint_system.pipelines.extraction_pipeline import ExtractionPipeline

__all__ = ["ExtractionPipeline"]
```
  </action>
  <verify>
```bash
uv run python -c "
from osint_system.pipelines import ExtractionPipeline
pipeline = ExtractionPipeline()
print(f'Pipeline batch size: {pipeline.batch_size}')
print(f'Has article_store: {pipeline.article_store is not None}')
print(f'Has extraction_agent: {pipeline.extraction_agent is not None}')
print(f'Has consolidator: {pipeline.consolidator is not None}')
print('Pipeline imports OK')
"
```
  </verify>
  <done>ExtractionPipeline wires ArticleStore to FactExtractionAgent to FactConsolidator</done>
</task>

<task type="auto">
  <name>Task 2: Add pipeline tests</name>
  <files>
    tests/pipelines/__init__.py
    tests/pipelines/test_extraction_pipeline.py
  </files>
  <action>
Create comprehensive tests for ExtractionPipeline.

**Test cases:**
1. Pipeline initialization with defaults
2. Article-to-content transformation
3. Empty investigation handling
4. Single article processing
5. Batch processing with mock extraction
6. Error handling (failed article extraction)
7. Pipeline status reporting
8. End-to-end flow with mock data

**Testing approach:**
- Mock FactExtractionAgent to avoid Gemini API calls
- Create test articles in ArticleStore format
- Verify facts flow through to FactStore

**Example test structure:**
```python
import pytest
from unittest.mock import MagicMock, AsyncMock, patch
from osint_system.pipelines import ExtractionPipeline
from osint_system.data_management.article_store import ArticleStore
from osint_system.data_management.fact_store import FactStore

@pytest.fixture
def mock_extraction_agent():
    """Mock agent that returns predictable facts."""
    agent = MagicMock()
    agent.sift = AsyncMock(return_value=[
        {
            "fact_id": "f1",
            "content_hash": "hash1",
            "claim": {"text": "Test fact", "assertion_type": "statement"}
        }
    ])
    return agent

@pytest.fixture
def sample_articles():
    return [
        {
            "url": "https://example.com/article1",
            "title": "Test Article",
            "content": "Putin visited Beijing in March 2024.",
            "published_date": "2024-03-15",
            "source": {"name": "Test News"},
            "stored_at": "2024-03-15T12:00:00Z"
        }
    ]

@pytest.mark.asyncio
async def test_article_to_content_transformation():
    pipeline = ExtractionPipeline()
    article = {
        "url": "https://example.com/test",
        "title": "Breaking News",
        "content": "Something happened.",
        "published_date": "2024-01-01",
        "source": {"name": "News Org", "type": "wire_service"}
    }
    content = pipeline._article_to_content(article)
    assert content["text"] == "Breaking News\n\nSomething happened."
    assert content["source_id"] == "https://example.com/test"
    assert content["source_type"] == "wire_service"
```

Create tests/pipelines/__init__.py (empty).
  </action>
  <verify>
`uv run python -m pytest tests/pipelines/test_extraction_pipeline.py -v`
  </verify>
  <done>All pipeline tests pass with mocked components</done>
</task>

</tasks>

<verification>
```bash
# All imports work
uv run python -c "
from osint_system.pipelines import ExtractionPipeline
from osint_system.data_management.article_store import ArticleStore
from osint_system.data_management.fact_store import FactStore
from osint_system.agents.sifters import FactExtractionAgent, FactConsolidator
print('All pipeline imports OK')
"

# Tests pass
uv run python -m pytest tests/pipelines/test_extraction_pipeline.py -v

# Integration check (manual verification)
uv run python -c "
import asyncio
from osint_system.pipelines import ExtractionPipeline
from osint_system.data_management.article_store import ArticleStore

async def integration():
    # Create stores
    article_store = ArticleStore()

    # Add test article
    await article_store.save_articles('test-inv', [{
        'url': 'https://example.com/test',
        'title': 'Test Article',
        'content': 'This is test content for extraction.',
        'source': {'name': 'Test'}
    }])

    # Create pipeline with mock agent (no Gemini call)
    from unittest.mock import MagicMock, AsyncMock
    mock_agent = MagicMock()
    mock_agent.sift = AsyncMock(return_value=[{
        'fact_id': 'f1',
        'content_hash': 'abc',
        'claim': {'text': 'Test fact'}
    }])

    pipeline = ExtractionPipeline(
        article_store=article_store,
        extraction_agent=mock_agent
    )

    # Run pipeline
    result = await pipeline.process_investigation('test-inv')
    print(f'Articles processed: {result[\"articles_processed\"]}')
    print(f'Facts extracted: {result[\"facts_extracted\"]}')
    print(f'Facts consolidated: {result[\"facts_consolidated\"]}')
    print('Integration OK')

asyncio.run(integration())
"
```
</verification>

<success_criteria>
- ExtractionPipeline instantiates with all required components
- Articles from ArticleStore transform to extraction format
- FactExtractionAgent receives properly formatted content
- Extracted facts flow to FactConsolidator
- Consolidated facts stored in FactStore
- Pipeline handles empty investigations gracefully
- Pipeline reports processing statistics
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/06-fact-extraction-pipeline/06-04-SUMMARY.md`
</output>
