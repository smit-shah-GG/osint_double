---
phase: 05-extended-crawler-cohort
plan: 03
type: execute
---

<objective>
Implement document crawler for PDF and web document extraction.

Purpose: Enable extraction of text from PDFs and documents with high-quality content parsing.
Output: Working DocumentCrawler that handles PDFs and web documents with metadata extraction.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-extended-crawler-cohort/05-CONTEXT.md
@.planning/phases/05-extended-crawler-cohort/05-RESEARCH.md
@.planning/phases/05-extended-crawler-cohort/05-02-SUMMARY.md
@osint_system/agents/crawlers/document_scraper_agent.py
@osint_system/agents/crawlers/base_crawler.py
@osint_system/agents/crawlers/extractors/article_extractor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install document processing dependencies</name>
  <files>requirements.txt</files>
  <action>Add document processing libraries: pypdfium2>=4.25.0 for PDF text extraction (best quality per research), pdfplumber>=0.10.0 for table extraction fallback, trafilatura>=1.6.0 for web content extraction (F1 score 0.958). Also add beautifulsoup4>=4.12.0 as a parsing helper. Install with uv pip install -r requirements.txt.</action>
  <verify>uv pip freeze | grep -E "pypdfium2|pdfplumber|trafilatura|beautifulsoup4" shows all installed</verify>
  <done>All document processing libraries installed successfully</done>
</task>

<task type="auto">
  <name>Task 2: Implement DocumentCrawler class</name>
  <files>osint_system/agents/crawlers/document_scraper_agent.py</files>
  <action>Rename DocumentScraperAgent to DocumentCrawler. Add methods: fetch_document(url) using httpx for downloading, extract_pdf_content(pdf_bytes) using pypdfium2 first then pdfplumber fallback for tables, extract_web_content(html, url) using trafilatura.extract() with include_tables=True. Add is_pdf(url) helper checking content-type and extension. Implement process_document(url) that routes to appropriate extractor. Include metadata extraction: title, author, date from trafilatura.extract_metadata(). Return structured format with content, metadata, and document_type.</action>
  <verify>python -c "from osint_system.agents.crawlers.document_scraper_agent import DocumentCrawler; print('Import successful')"</verify>
  <done>DocumentCrawler implements PDF and web extraction with metadata</done>
</task>

<task type="auto">
  <name>Task 3: Add content quality filtering</name>
  <files>osint_system/agents/crawlers/document_scraper_agent.py</files>
  <action>Add quality filtering to DocumentCrawler. Implement min_content_length check (default 500 chars). Add authority scoring based on domain (government/edu = 0.9, org = 0.7, others = 0.5). Implement extract_with_fallback() following Pattern 3 from RESEARCH.md - try trafilatura, fallback to readability if available, then raw text. Add language detection to filter non-English content (optional). Return None for low-quality content. Store authority_score in metadata.</action>
  <verify>grep "min_content_length\|authority_score" osint_system/agents/crawlers/document_scraper_agent.py shows filtering</verify>
  <done>Quality filtering and authority scoring implemented</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] Document processing libraries installed
- [ ] PDF extraction working with pypdfium2
- [ ] Web content extraction with trafilatura
- [ ] Quality filtering excludes short content
- [ ] Authority scoring based on domain
</verification>

<success_criteria>
- All tasks completed
- DocumentCrawler handles PDFs and web documents
- Content quality filtering works
- Metadata extraction functional
</success_criteria>

<output>
After completion, create `.planning/phases/05-extended-crawler-cohort/05-03-SUMMARY.md`:

# Phase 5 Plan 3: Document Crawler Summary

**Document extraction pipeline established for PDFs and web documents.**

## Accomplishments

- Installed document processing libraries (pypdfium2, trafilatura)
- Implemented DocumentCrawler with PDF and web extraction
- Added content quality filtering and authority scoring
- Metadata extraction for documents

## Files Created/Modified

- `requirements.txt` - Added document processing dependencies
- `osint_system/agents/crawlers/document_scraper_agent.py` - Implemented DocumentCrawler

## Decisions Made

- pypdfium2 as primary PDF extractor (best quality)
- trafilatura for web content (F1 0.958)
- Minimum content length 500 characters
- Domain-based authority scoring

## Issues Encountered

[Any PDF parsing issues or extraction failures]

## Next Step

Ready for 05-04-PLAN.md - Web scraper enhancement
</output>