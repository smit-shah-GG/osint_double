---
phase: 05-extended-crawler-cohort
plan: 06
type: execute
---

<objective>
Test and integrate the extended crawler cohort end-to-end.

Purpose: Ensure all crawlers work together effectively through the message bus with proper coordination.
Output: Fully tested crawler cohort with integration tests and example investigation flow.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-extended-crawler-cohort/05-CONTEXT.md
@.planning/phases/05-extended-crawler-cohort/05-05-SUMMARY.md
@osint_system/agents/planning_agent.py
@osint_system/utils/message_bus.py
@tests/integration/test_crawler_integration.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create crawler cohort integration test</name>
  <files>tests/integration/test_extended_crawler_cohort.py</files>
  <action>Create comprehensive integration test. Test RedditCrawler + DocumentCrawler + HybridWebCrawler working together. Mock investigation request with keywords "Ukraine conflict". Test that: RedditCrawler finds relevant posts, DocumentCrawler extracts from linked PDFs, HybridWebCrawler handles news sites, URLManager prevents duplicate fetches, AuthorityScorer ranks sources correctly, ContextCoordinator shares discovered entities. Use asyncio.gather for parallel crawler execution. Mock external APIs where needed.</action>
  <verify>uv run pytest tests/integration/test_extended_crawler_cohort.py -v -s</verify>
  <done>Integration test covers crawler coordination scenarios</done>
</task>

<task type="auto">
  <name>Task 2: Update Planning Agent for crawler triggering</name>
  <files>osint_system/agents/planning_agent.py</files>
  <action>Update PlanningAgent to trigger extended crawlers. In decompose_objective(), detect keywords suggesting different source types: "Reddit"/"discussion" -> reddit.crawl, "document"/"report"/"PDF" -> document.crawl, general web -> web.crawl. Publish appropriate messages with investigation_id and keywords. Add source_types parameter to control which crawlers to activate. Update to listen for crawler completion messages and aggregate results. Follow existing pattern from news crawler integration.</action>
  <verify>grep "reddit.crawl\|document.crawl\|web.crawl" osint_system/agents/planning_agent.py</verify>
  <done>Planning Agent triggers appropriate crawlers based on task type</done>
</task>

<task type="auto">
  <name>Task 3: Create example investigation script</name>
  <files>examples/run_extended_investigation.py</files>
  <action>Create script demonstrating full crawler cohort. Initialize MessageBus and all crawlers (Reddit, Document, Web). Create sample investigation for "recent geopolitical event". Show Planning Agent decomposing into subtasks, triggering appropriate crawlers, crawlers sharing discovered entities, results being aggregated. Add logging to show crawler coordination. Include command-line argument for investigation topic. Print final collected articles/posts/documents with authority scores.</action>
  <verify>uv run python examples/run_extended_investigation.py "sample topic"</verify>
  <done>Example script demonstrates crawler cohort in action</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] Integration tests pass
- [ ] Planning Agent triggers all crawler types
- [ ] Crawlers coordinate via message bus
- [ ] Example script runs successfully
</verification>

<success_criteria>
- All tasks completed
- End-to-end crawler coordination tested
- Planning Agent integration complete
- Example demonstrates full workflow
- Phase 5 complete
</success_criteria>

<output>
After completion, create `.planning/phases/05-extended-crawler-cohort/05-06-SUMMARY.md`:

# Phase 5 Plan 6: Integration Testing Summary

**Extended crawler cohort fully integrated and tested.**

## Accomplishments

- Created comprehensive integration tests
- Updated Planning Agent for multi-crawler triggering
- Built example investigation script
- Validated end-to-end crawler coordination

## Files Created/Modified

- `tests/integration/test_extended_crawler_cohort.py` - Integration tests
- `osint_system/agents/planning_agent.py` - Multi-crawler triggering
- `examples/run_extended_investigation.py` - Demo script

## Decisions Made

- Keyword-based crawler selection in Planning Agent
- Parallel crawler execution with asyncio.gather
- Source type detection from task description

## Issues Encountered

[Any integration or coordination issues]

## Next Step

Phase 5 complete. Ready for Phase 6: Fact Extraction Pipeline
</output>