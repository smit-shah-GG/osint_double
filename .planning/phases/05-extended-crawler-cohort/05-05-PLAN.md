---
phase: 05-extended-crawler-cohort
plan: 05
type: execute
---

<objective>
Implement crawler coordination with shared context and deduplication.

Purpose: Enable crawlers to work together intelligently, sharing discoveries and avoiding duplicates.
Output: Coordination system with URL management, authority scoring, and shared entity tracking.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-extended-crawler-cohort/05-CONTEXT.md
@.planning/phases/05-extended-crawler-cohort/05-RESEARCH.md
@.planning/phases/05-extended-crawler-cohort/05-04-SUMMARY.md
@osint_system/agents/crawlers/deduplication/dedup_engine.py
@osint_system/utils/message_bus.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Enhance URL deduplication manager</name>
  <files>osint_system/agents/crawlers/coordination/url_manager.py</files>
  <action>Create URLManager class using yarl for URL normalization. Implement normalize_url() that handles protocol variations, removes tracking parameters, normalizes paths. Add seen_urls set for O(1) duplicate detection. Create add_url(url, investigation_id) and is_duplicate(url, investigation_id) methods. Include investigation-scoped tracking (same URL can appear in different investigations). Add extract_domain(url) for authority scoring. Use yarl.URL for immutable URL handling per RESEARCH.md recommendations.</action>
  <verify>python -c "from osint_system.agents.crawlers.coordination.url_manager import URLManager; mgr = URLManager(); print('Created')"</verify>
  <done>URLManager handles URL normalization and deduplication</done>
</task>

<task type="auto">
  <name>Task 2: Implement authority scoring system</name>
  <files>osint_system/agents/crawlers/coordination/authority_scorer.py</files>
  <action>Create AuthorityScorer with domain-based scoring. Define AUTHORITY_DOMAINS dict: reuters.com/apnews.com/bbc.com = 0.9, .gov/.edu = 0.85, .org = 0.7, reddit.com/twitter.com = 0.3, default = 0.5. Implement calculate_score(url, metadata) combining domain score with metadata signals (author verification, publication date, engagement metrics). Add source_type weights: official = 1.0, news = 0.9, social = 0.3. Return composite score 0-1. Include get_domain_category() for reporting.</action>
  <verify>grep "calculate_score\|AUTHORITY_DOMAINS" osint_system/agents/crawlers/coordination/authority_scorer.py</verify>
  <done>AuthorityScorer implements domain and signal-based scoring</done>
</task>

<task type="auto">
  <name>Task 3: Create shared context coordinator</name>
  <files>osint_system/agents/crawlers/coordination/context_coordinator.py</files>
  <action>Create ContextCoordinator for crawler collaboration. Implement entity tracking: discovered_entities dict mapping entities to source URLs. Add topic expansion: related_topics set that grows as crawlers find connections. Create share_discovery(entity, source, investigation_id) to broadcast via message bus. Implement get_related_sources(entity) returning URLs that mentioned entity. Add cross_reference(content, known_entities) to find entity mentions. Integrate with MessageBus to publish "context.update" messages when new entities/topics discovered.</action>
  <verify>python -c "from osint_system.agents.crawlers.coordination.context_coordinator import ContextCoordinator; print('Import successful')"</verify>
  <done>ContextCoordinator enables crawler collaboration through shared discoveries</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] URL normalization and deduplication working
- [ ] Authority scoring returns appropriate scores
- [ ] Context sharing via message bus functional
- [ ] Entity tracking across crawlers implemented
</verification>

<success_criteria>
- All tasks completed
- URL deduplication prevents redundant crawling
- Authority scoring differentiates source quality
- Crawlers can share discovered entities
</success_criteria>

<output>
After completion, create `.planning/phases/05-extended-crawler-cohort/05-05-SUMMARY.md`:

# Phase 5 Plan 5: Crawler Coordination Summary

**Coordination system enables intelligent crawler collaboration.**

## Accomplishments

- Created URL manager with yarl normalization
- Implemented authority scoring system
- Built shared context coordinator
- Enabled entity tracking across crawlers

## Files Created/Modified

- `osint_system/agents/crawlers/coordination/url_manager.py` - URL deduplication
- `osint_system/agents/crawlers/coordination/authority_scorer.py` - Authority scoring
- `osint_system/agents/crawlers/coordination/context_coordinator.py` - Shared context

## Decisions Made

- yarl for URL normalization (immutable URLs)
- Domain-based authority with metadata signals
- Entity-based context sharing
- Investigation-scoped deduplication

## Issues Encountered

[Any coordination or messaging issues]

## Next Step

Ready for 05-06-PLAN.md - Integration testing
</output>