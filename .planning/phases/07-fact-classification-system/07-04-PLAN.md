---
phase: 07-fact-classification-system
plan: 04
type: execute
wave: 4
depends_on: ["07-03"]
files_modified:
  - osint_system/agents/sifters/classification/__init__.py
  - osint_system/agents/sifters/classification/impact_assessor.py
  - osint_system/agents/sifters/classification/anomaly_detector.py
  - osint_system/agents/sifters/fact_classification_agent.py
  - tests/agents/sifters/classification/test_impact_assessor.py
  - tests/agents/sifters/classification/test_anomaly_detector.py
autonomous: true

must_haves:
  truths:
    - "Impact assessment considers entity significance AND event type"
    - "Critical tier for world leaders, military actions, treaties, sanctions"
    - "Less-critical tier for routine activities, minor entities"
    - "AnomalyDetector finds contradictions for ANOMALY flag input"
    - "FactClassificationAgent integrates DubiousDetector, ImpactAssessor, and AnomalyDetector"
    - "classify_investigation method enables full anomaly detection across all facts"
  artifacts:
    - path: "osint_system/agents/sifters/classification/impact_assessor.py"
      provides: "ImpactAssessor for critical vs less-critical tier determination"
      exports: ["ImpactAssessor", "ImpactResult"]
      min_lines: 150
    - path: "osint_system/agents/sifters/classification/anomaly_detector.py"
      provides: "AnomalyDetector for contradiction detection between facts"
      exports: ["AnomalyDetector", "Contradiction"]
      min_lines: 120
  key_links:
    - from: "osint_system/agents/sifters/fact_classification_agent.py"
      to: "osint_system/agents/sifters/classification/dubious_detector.py"
      via: "DubiousDetector for _detect_dubious"
      pattern: "from.*classification.*import.*DubiousDetector"
    - from: "osint_system/agents/sifters/fact_classification_agent.py"
      to: "osint_system/agents/sifters/classification/impact_assessor.py"
      via: "ImpactAssessor for _assess_impact"
      pattern: "from.*classification.*import.*ImpactAssessor"
    - from: "osint_system/agents/sifters/classification/anomaly_detector.py"
      to: "osint_system/data_management/fact_store.py"
      via: "FactStore for cross-fact contradiction detection"
      pattern: "from.*fact_store.*import.*FactStore"
---

<objective>
Implement ImpactAssessor, AnomalyDetector, and integrate all classification components into FactClassificationAgent.

Purpose: Complete the fact classification system with impact tier determination and contradiction detection. ImpactAssessor determines critical vs less-critical based on entity significance and event type. AnomalyDetector provides contradiction input for the ANOMALY dubious flag (from Plan 03).

Output: Full classification pipeline with DubiousDetector + ImpactAssessor + AnomalyDetector integrated into FactClassificationAgent.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-fact-classification-system/07-CONTEXT.md
@.planning/phases/07-fact-classification-system/07-01-SUMMARY.md
@.planning/phases/07-fact-classification-system/07-02-SUMMARY.md
@.planning/phases/07-fact-classification-system/07-03-SUMMARY.md

# Schemas we use
@osint_system/data_management/schemas/fact_schema.py
@osint_system/data_management/schemas/classification_schema.py
@osint_system/config/source_credibility.py
@osint_system/config/prompts/classification_prompts.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement ImpactAssessor</name>
  <files>osint_system/agents/sifters/classification/impact_assessor.py</files>
  <action>
Create ImpactAssessor for determining critical vs less-critical impact tier.

**impact_assessor.py:**
```python
"""Impact assessment for fact classification per Phase 7 CONTEXT.md.

Impact tier determination based on:
- Entity significance (world leaders > officials > others)
- Event type (military action > diplomatic > routine)
- Investigation context (relevance to objective)

Impact is orthogonal to trust - a fact can be CRITICAL and DUBIOUS.
High-impact dubious facts get priority verification in Phase 8.
"""

import re
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

from loguru import logger

from osint_system.config.prompts.classification_prompts import (
    CRITICAL_ENTITY_PATTERNS,
    CRITICAL_EVENT_KEYWORDS,
)
from osint_system.config.source_credibility import (
    ENTITY_SIGNIFICANCE,
    EVENT_TYPE_SIGNIFICANCE,
)
from osint_system.data_management.schemas import ImpactTier


@dataclass
class ImpactResult:
    """Result of impact assessment."""
    tier: ImpactTier
    score: float  # 0.0-1.0 impact score
    entity_contribution: float
    event_contribution: float
    reasoning: str


class ImpactAssessor:
    """
    Assesses impact tier for facts based on geopolitical significance.

    Per CONTEXT.md:
    - Impact based on entity significance AND event type
    - Investigation-relative: same fact may be critical in one investigation
    - Orthogonal to trust: critical facts can also be dubious

    Usage:
        assessor = ImpactAssessor()
        result = assessor.assess(fact, investigation_context)
    """

    # Threshold for CRITICAL tier (0.6 = significant entity OR event)
    CRITICAL_THRESHOLD = 0.6

    def __init__(
        self,
        critical_threshold: float = CRITICAL_THRESHOLD,
        entity_weight: float = 0.5,
        event_weight: float = 0.5,
    ):
        """
        Initialize impact assessor.

        Args:
            critical_threshold: Score above which fact is CRITICAL (default 0.6)
            entity_weight: Weight for entity contribution (default 0.5)
            event_weight: Weight for event contribution (default 0.5)
        """
        self.critical_threshold = critical_threshold
        self.entity_weight = entity_weight
        self.event_weight = event_weight
        self.entity_patterns = [re.compile(p, re.IGNORECASE) for p in CRITICAL_ENTITY_PATTERNS]
        self.logger = logger.bind(component="ImpactAssessor")

    def assess(
        self,
        fact: Dict[str, Any],
        investigation_context: Optional[Dict[str, Any]] = None,
    ) -> ImpactResult:
        """
        Assess impact tier for a fact.

        Args:
            fact: ExtractedFact dict
            investigation_context: Optional investigation metadata for context

        Returns:
            ImpactResult with tier, score, and reasoning
        """
        # Assess entity significance
        entity_score, entity_reason = self._assess_entities(fact)

        # Assess event type significance
        event_score, event_reason = self._assess_event_type(fact)

        # Combine scores
        combined_score = (
            self.entity_weight * entity_score +
            self.event_weight * event_score
        )

        # Apply investigation context if available
        if investigation_context:
            context_boost = self._apply_investigation_context(
                fact, investigation_context
            )
            combined_score = min(1.0, combined_score + context_boost)

        # Determine tier
        if combined_score >= self.critical_threshold:
            tier = ImpactTier.CRITICAL
        else:
            tier = ImpactTier.LESS_CRITICAL

        reasoning_parts = []
        if entity_reason:
            reasoning_parts.append(f"Entity: {entity_reason}")
        if event_reason:
            reasoning_parts.append(f"Event: {event_reason}")
        reasoning = "; ".join(reasoning_parts) or "Default assessment"

        result = ImpactResult(
            tier=tier,
            score=combined_score,
            entity_contribution=entity_score,
            event_contribution=event_score,
            reasoning=reasoning,
        )

        self.logger.debug(
            f"Impact assessment: {tier.value}",
            fact_id=fact.get("fact_id", "unknown")[:20],
            score=combined_score,
        )

        return result

    def _assess_entities(self, fact: Dict[str, Any]) -> tuple[float, str]:
        """
        Assess entity significance.

        Returns:
            (significance_score, reasoning)
        """
        entities = fact.get("entities", [])
        claim_text = fact.get("claim", {}).get("text", "")

        if not entities and not claim_text:
            return 0.3, "No entities"

        max_significance = 0.3
        significant_entity = None

        # Check explicit entities
        for entity in entities:
            entity_type = entity.get("type", "unknown")
            entity_text = entity.get("text", "").lower()
            canonical = entity.get("canonical", "").lower()

            # Look up significance
            significance = self._get_entity_significance(entity_type, entity_text, canonical)
            if significance > max_significance:
                max_significance = significance
                significant_entity = entity.get("canonical") or entity.get("text")

        # Check claim text for pattern matches
        for pattern in self.entity_patterns:
            if pattern.search(claim_text):
                if max_significance < 0.8:
                    max_significance = 0.8
                    match = pattern.search(claim_text)
                    if match and not significant_entity:
                        significant_entity = match.group(0)
                break

        reason = f"{significant_entity} (score={max_significance:.2f})" if significant_entity else "Low significance entities"
        return max_significance, reason

    def _get_entity_significance(
        self,
        entity_type: str,
        entity_text: str,
        canonical: str,
    ) -> float:
        """Get significance score for an entity."""
        # Check canonical name against known significant entities
        combined = f"{entity_text} {canonical}".lower()

        # World leaders
        world_leaders = ["putin", "biden", "xi", "modi", "macron", "scholz", "sunak"]
        if any(leader in combined for leader in world_leaders):
            return ENTITY_SIGNIFICANCE.get("world_leader", 1.0)

        # Military/government keywords
        if any(kw in combined for kw in ["president", "prime minister", "minister", "general"]):
            return ENTITY_SIGNIFICANCE.get("senior_official", 0.8)

        # Organizations
        orgs = ["nato", "un", "eu", "g7", "pentagon", "kremlin"]
        if any(org in combined for org in orgs):
            return ENTITY_SIGNIFICANCE.get("organization", 0.6)

        # Entity type fallback
        type_mapping = {
            "PERSON": 0.4,
            "ORGANIZATION": 0.4,
            "LOCATION": 0.3,
            "EVENT": 0.5,
        }
        return type_mapping.get(entity_type.upper(), 0.3)

    def _assess_event_type(self, fact: Dict[str, Any]) -> tuple[float, str]:
        """
        Assess event type significance.

        Returns:
            (significance_score, reasoning)
        """
        claim = fact.get("claim", {})
        claim_text = claim.get("text", "").lower()
        claim_type = claim.get("claim_type", "event")

        # Check for critical event keywords
        matched_keywords = []
        for keyword in CRITICAL_EVENT_KEYWORDS:
            if keyword in claim_text:
                matched_keywords.append(keyword)

        if matched_keywords:
            # Map keywords to event types
            if any(kw in matched_keywords for kw in ["attack", "strike", "invasion", "war", "military", "nuclear", "missile"]):
                return EVENT_TYPE_SIGNIFICANCE.get("military_action", 1.0), f"Military: {matched_keywords[:3]}"
            if any(kw in matched_keywords for kw in ["treaty", "agreement", "sanction", "embargo"]):
                return EVENT_TYPE_SIGNIFICANCE.get("treaty_agreement", 0.9), f"Diplomatic: {matched_keywords[:3]}"
            if any(kw in matched_keywords for kw in ["summit", "diplomatic", "negotiation"]):
                return EVENT_TYPE_SIGNIFICANCE.get("diplomatic_meeting", 0.7), f"Meeting: {matched_keywords[:3]}"
            if any(kw in matched_keywords for kw in ["election", "coup", "assassination", "crisis"]):
                return EVENT_TYPE_SIGNIFICANCE.get("policy_announcement", 0.8), f"Major event: {matched_keywords[:3]}"

        # Fall back to claim type
        claim_type_scores = {
            "event": 0.5,
            "state": 0.3,
            "relationship": 0.4,
            "prediction": 0.6,
            "planned": 0.5,
        }
        score = claim_type_scores.get(claim_type, 0.3)
        return score, f"Claim type: {claim_type}"

    def _apply_investigation_context(
        self,
        fact: Dict[str, Any],
        context: Dict[str, Any],
    ) -> float:
        """
        Apply investigation context boost.

        Context can include:
        - objective_keywords: Keywords that boost relevance
        - temporal_focus: If recency matters
        - entity_focus: Specific entities of interest

        Returns:
            Context boost (0.0-0.2)
        """
        boost = 0.0

        # Keyword matching
        objective_keywords = context.get("objective_keywords", [])
        claim_text = fact.get("claim", {}).get("text", "").lower()
        if any(kw.lower() in claim_text for kw in objective_keywords):
            boost += 0.1

        # Entity focus matching
        entity_focus = context.get("entity_focus", [])
        entities = fact.get("entities", [])
        for entity in entities:
            entity_text = entity.get("text", "").lower()
            canonical = entity.get("canonical", "").lower()
            if any(focus.lower() in entity_text or focus.lower() in canonical for focus in entity_focus):
                boost += 0.1
                break

        return min(0.2, boost)

    def bulk_assess(
        self,
        facts: List[Dict[str, Any]],
        investigation_context: Optional[Dict[str, Any]] = None,
    ) -> List[ImpactResult]:
        """Assess impact for multiple facts."""
        return [self.assess(fact, investigation_context) for fact in facts]


__all__ = ["ImpactAssessor", "ImpactResult"]
```

**Update classification/__init__.py:**
Add ImpactAssessor exports.
  </action>
  <verify>
```bash
uv run python -c "
from osint_system.agents.sifters.classification import ImpactAssessor
from osint_system.data_management.schemas import ImpactTier

assessor = ImpactAssessor()

# Test critical fact (world leader + military action)
critical_fact = {
    'fact_id': 'crit-1',
    'claim': {'text': '[E1:Putin] ordered military strike on [E2:Kyiv]', 'claim_type': 'event'},
    'entities': [
        {'id': 'E1', 'text': 'Putin', 'type': 'PERSON', 'canonical': 'Vladimir Putin'},
        {'id': 'E2', 'text': 'Kyiv', 'type': 'LOCATION'}
    ]
}
result = assessor.assess(critical_fact)
print(f'Critical test: tier={result.tier.value}, score={result.score:.2f}')
print(f'  Entity: {result.entity_contribution:.2f}, Event: {result.event_contribution:.2f}')
print(f'  Reasoning: {result.reasoning}')
assert result.tier == ImpactTier.CRITICAL, 'Should be CRITICAL'

# Test less-critical fact (routine)
routine_fact = {
    'fact_id': 'routine-1',
    'claim': {'text': 'Official statement was released', 'claim_type': 'event'},
    'entities': [
        {'id': 'E1', 'text': 'spokesperson', 'type': 'PERSON'}
    ]
}
result = assessor.assess(routine_fact)
print(f'Routine test: tier={result.tier.value}, score={result.score:.2f}')
assert result.tier == ImpactTier.LESS_CRITICAL, 'Should be LESS_CRITICAL'

print('ImpactAssessor OK')
"
```
  </verify>
  <done>ImpactAssessor with entity/event significance and investigation context</done>
</task>

<task type="auto">
  <name>Task 2: Implement AnomalyDetector for contradiction detection</name>
  <files>osint_system/agents/sifters/classification/anomaly_detector.py</files>
  <action>
Create AnomalyDetector for detecting contradictions between facts.

**anomaly_detector.py:**
```python
"""Anomaly detection for fact contradictions per Phase 7 CONTEXT.md.

Per CONTEXT.md:
- Tiered approach: within-investigation for all facts, cross-investigation for critical-tier only
- Contradiction triggers ANOMALY flag for Phase 8 arbitration
- Does NOT attempt premature resolution - Phase 8 handles arbitration

Contradiction types:
- Direct negation: "Russia attacked" vs "Russia did not attack"
- Numeric disagreement: "50 casualties" vs "200 casualties"
- Temporal conflict: "Happened on Monday" vs "Happened on Tuesday"
- Attribution conflict: "Putin said X" vs "Putin denied X"
"""

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Set

from loguru import logger


@dataclass
class Contradiction:
    """A detected contradiction between two facts."""
    fact_id_a: str
    fact_id_b: str
    contradiction_type: str  # negation, numeric, temporal, attribution
    confidence: float
    details: Dict[str, Any] = field(default_factory=dict)


class AnomalyDetector:
    """
    Detects contradictions between facts for ANOMALY classification.

    Per CONTEXT.md:
    - Within-investigation detection for all facts
    - Cross-investigation detection for critical-tier only (optional)
    - Tracks conflicts without attempting resolution

    Usage:
        detector = AnomalyDetector()
        contradictions = await detector.find_contradictions(
            fact, all_facts_in_investigation
        )
    """

    # Negation indicators
    NEGATION_WORDS = {"not", "no", "never", "denied", "rejected", "refused", "false", "untrue"}
    NEGATION_PREFIXES = {"un", "non", "dis"}

    def __init__(self, min_confidence: float = 0.5):
        """
        Initialize anomaly detector.

        Args:
            min_confidence: Minimum confidence to report contradiction (default 0.5)
        """
        self.min_confidence = min_confidence
        self.logger = logger.bind(component="AnomalyDetector")

    async def find_contradictions(
        self,
        target_fact: Dict[str, Any],
        comparison_facts: List[Dict[str, Any]],
    ) -> List[Contradiction]:
        """
        Find facts that contradict the target fact.

        Args:
            target_fact: The fact to check for contradictions
            comparison_facts: List of facts to compare against

        Returns:
            List of Contradiction objects
        """
        contradictions = []
        target_id = target_fact.get("fact_id", "unknown")

        for other_fact in comparison_facts:
            other_id = other_fact.get("fact_id", "unknown")

            # Skip self-comparison
            if target_id == other_id:
                continue

            # Check for various contradiction types
            contradiction = self._detect_contradiction(target_fact, other_fact)
            if contradiction and contradiction.confidence >= self.min_confidence:
                contradictions.append(contradiction)

        self.logger.debug(
            f"Found {len(contradictions)} contradictions for {target_id[:20]}",
            comparison_count=len(comparison_facts),
        )

        return contradictions

    def _detect_contradiction(
        self,
        fact_a: Dict[str, Any],
        fact_b: Dict[str, Any],
    ) -> Optional[Contradiction]:
        """
        Detect if two facts contradict each other.

        Returns:
            Contradiction if found, None otherwise
        """
        # Get claim texts
        claim_a = fact_a.get("claim", {}).get("text", "")
        claim_b = fact_b.get("claim", {}).get("text", "")

        if not claim_a or not claim_b:
            return None

        # Check for direct negation
        negation_result = self._check_negation(claim_a, claim_b, fact_a, fact_b)
        if negation_result:
            return negation_result

        # Check for assertion type contradiction (statement vs denial)
        assertion_result = self._check_assertion_contradiction(fact_a, fact_b)
        if assertion_result:
            return assertion_result

        # Check for numeric disagreement
        numeric_result = self._check_numeric_contradiction(fact_a, fact_b)
        if numeric_result:
            return numeric_result

        # Check for temporal conflict
        temporal_result = self._check_temporal_contradiction(fact_a, fact_b)
        if temporal_result:
            return temporal_result

        return None

    def _check_negation(
        self,
        claim_a: str,
        claim_b: str,
        fact_a: Dict[str, Any],
        fact_b: Dict[str, Any],
    ) -> Optional[Contradiction]:
        """Check for direct negation between claims."""
        words_a = set(claim_a.lower().split())
        words_b = set(claim_b.lower().split())

        # Check if one has negation words that the other doesn't
        neg_in_a = bool(words_a & self.NEGATION_WORDS)
        neg_in_b = bool(words_b & self.NEGATION_WORDS)

        # If same negation status, not a direct contradiction
        if neg_in_a == neg_in_b:
            return None

        # Check if claims share significant content
        # Remove common stop words
        stop_words = {"the", "a", "an", "is", "are", "was", "were", "in", "on", "at", "to", "for"}
        content_a = words_a - stop_words - self.NEGATION_WORDS
        content_b = words_b - stop_words - self.NEGATION_WORDS

        overlap = content_a & content_b
        if len(overlap) < 2:
            # Not enough shared content to be about same thing
            return None

        confidence = min(0.9, 0.5 + len(overlap) * 0.1)

        return Contradiction(
            fact_id_a=fact_a.get("fact_id", "unknown"),
            fact_id_b=fact_b.get("fact_id", "unknown"),
            contradiction_type="negation",
            confidence=confidence,
            details={
                "negation_in": "claim_a" if neg_in_a else "claim_b",
                "shared_content": list(overlap),
            },
        )

    def _check_assertion_contradiction(
        self,
        fact_a: Dict[str, Any],
        fact_b: Dict[str, Any],
    ) -> Optional[Contradiction]:
        """Check for contradiction between statement and denial."""
        type_a = fact_a.get("claim", {}).get("assertion_type", "statement")
        type_b = fact_b.get("claim", {}).get("assertion_type", "statement")

        # One statement, one denial about same entities
        if not ((type_a == "statement" and type_b == "denial") or
                (type_a == "denial" and type_b == "statement")):
            return None

        # Check entity overlap
        entities_a = {e.get("canonical") or e.get("text") for e in fact_a.get("entities", [])}
        entities_b = {e.get("canonical") or e.get("text") for e in fact_b.get("entities", [])}

        overlap = entities_a & entities_b
        if not overlap:
            return None

        return Contradiction(
            fact_id_a=fact_a.get("fact_id", "unknown"),
            fact_id_b=fact_b.get("fact_id", "unknown"),
            contradiction_type="attribution",
            confidence=0.8,
            details={
                "statement_fact": fact_a.get("fact_id") if type_a == "statement" else fact_b.get("fact_id"),
                "denial_fact": fact_b.get("fact_id") if type_a == "statement" else fact_a.get("fact_id"),
                "shared_entities": list(overlap),
            },
        )

    def _check_numeric_contradiction(
        self,
        fact_a: Dict[str, Any],
        fact_b: Dict[str, Any],
    ) -> Optional[Contradiction]:
        """Check for numeric value disagreement."""
        numeric_a = fact_a.get("numeric")
        numeric_b = fact_b.get("numeric")

        if not numeric_a or not numeric_b:
            return None

        # Get normalized values
        range_a = numeric_a.get("value_normalized")
        range_b = numeric_b.get("value_normalized")

        if not range_a or not range_b:
            # Compare original values if no normalization
            orig_a = numeric_a.get("value_original", "")
            orig_b = numeric_b.get("value_original", "")
            if orig_a and orig_b and orig_a != orig_b:
                # Check entity overlap to confirm same topic
                entities_a = {e.get("canonical") or e.get("text") for e in fact_a.get("entities", [])}
                entities_b = {e.get("canonical") or e.get("text") for e in fact_b.get("entities", [])}
                if entities_a & entities_b:
                    return Contradiction(
                        fact_id_a=fact_a.get("fact_id", "unknown"),
                        fact_id_b=fact_b.get("fact_id", "unknown"),
                        contradiction_type="numeric",
                        confidence=0.6,
                        details={
                            "value_a": orig_a,
                            "value_b": orig_b,
                        },
                    )
            return None

        # Check if ranges are disjoint
        min_a, max_a = range_a[0], range_a[1] if len(range_a) > 1 else range_a[0]
        min_b, max_b = range_b[0], range_b[1] if len(range_b) > 1 else range_b[0]

        # Ranges don't overlap = contradiction
        if max_a < min_b or max_b < min_a:
            return Contradiction(
                fact_id_a=fact_a.get("fact_id", "unknown"),
                fact_id_b=fact_b.get("fact_id", "unknown"),
                contradiction_type="numeric",
                confidence=0.8,
                details={
                    "range_a": range_a,
                    "range_b": range_b,
                },
            )

        return None

    def _check_temporal_contradiction(
        self,
        fact_a: Dict[str, Any],
        fact_b: Dict[str, Any],
    ) -> Optional[Contradiction]:
        """Check for temporal conflict."""
        temporal_a = fact_a.get("temporal")
        temporal_b = fact_b.get("temporal")

        if not temporal_a or not temporal_b:
            return None

        # Both need explicit precision to compare
        if (temporal_a.get("temporal_precision") != "explicit" or
            temporal_b.get("temporal_precision") != "explicit"):
            return None

        value_a = temporal_a.get("value", "")
        value_b = temporal_b.get("value", "")

        if not value_a or not value_b:
            return None

        # Same precision level needed for meaningful comparison
        precision_a = temporal_a.get("precision", "day")
        precision_b = temporal_b.get("precision", "day")

        if precision_a != precision_b:
            return None

        # Different values at same precision = potential conflict
        if value_a != value_b:
            # Check entity overlap
            entities_a = {e.get("canonical") or e.get("text") for e in fact_a.get("entities", [])}
            entities_b = {e.get("canonical") or e.get("text") for e in fact_b.get("entities", [])}

            if entities_a & entities_b:
                return Contradiction(
                    fact_id_a=fact_a.get("fact_id", "unknown"),
                    fact_id_b=fact_b.get("fact_id", "unknown"),
                    contradiction_type="temporal",
                    confidence=0.7,
                    details={
                        "temporal_a": value_a,
                        "temporal_b": value_b,
                        "precision": precision_a,
                    },
                )

        return None


__all__ = ["AnomalyDetector", "Contradiction"]
```

**Update classification/__init__.py:**
Add AnomalyDetector exports.
  </action>
  <verify>
```bash
uv run python -c "
import asyncio
from osint_system.agents.sifters.classification.anomaly_detector import AnomalyDetector

async def test():
    detector = AnomalyDetector()

    # Test negation contradiction
    fact_a = {
        'fact_id': 'a1',
        'claim': {'text': 'Russia attacked Ukraine', 'assertion_type': 'statement'},
        'entities': [{'text': 'Russia'}, {'text': 'Ukraine'}]
    }
    fact_b = {
        'fact_id': 'b1',
        'claim': {'text': 'Russia did not attack Ukraine', 'assertion_type': 'statement'},
        'entities': [{'text': 'Russia'}, {'text': 'Ukraine'}]
    }

    contradictions = await detector.find_contradictions(fact_a, [fact_b])
    print(f'Negation test: {len(contradictions)} contradictions')
    if contradictions:
        print(f'  Type: {contradictions[0].contradiction_type}, Confidence: {contradictions[0].confidence:.2f}')
    assert len(contradictions) >= 1, 'Should detect negation'

    # Test numeric contradiction
    fact_e = {
        'fact_id': 'e1',
        'claim': {'text': 'casualties'},
        'entities': [{'text': 'Battle'}],
        'numeric': {'value_normalized': [40, 60]}
    }
    fact_f = {
        'fact_id': 'f1',
        'claim': {'text': 'casualties'},
        'entities': [{'text': 'Battle'}],
        'numeric': {'value_normalized': [180, 220]}
    }

    contradictions = await detector.find_contradictions(fact_e, [fact_f])
    print(f'Numeric test: {len(contradictions)} contradictions')
    if contradictions:
        print(f'  Type: {contradictions[0].contradiction_type}, Ranges: {contradictions[0].details}')

    print('AnomalyDetector OK')

asyncio.run(test())
"
```
  </verify>
  <done>AnomalyDetector with multi-type contradiction detection</done>
</task>

<task type="auto">
  <name>Task 3: Integrate classification logic into FactClassificationAgent</name>
  <files>osint_system/agents/sifters/fact_classification_agent.py</files>
  <action>
Update FactClassificationAgent to use DubiousDetector, ImpactAssessor, and AnomalyDetector.

**Update imports at top:**
```python
from osint_system.agents.sifters.classification import (
    DubiousDetector,
    DubiousResult,
    ImpactAssessor,
    ImpactResult,
)
from osint_system.agents.sifters.classification.anomaly_detector import (
    AnomalyDetector,
    Contradiction,
)
```

**Add lazy initialization properties:**
```python
    @property
    def dubious_detector(self) -> DubiousDetector:
        """Lazy initialization of dubious detector."""
        if not hasattr(self, '_dubious_detector') or self._dubious_detector is None:
            self._dubious_detector = DubiousDetector()
        return self._dubious_detector

    @property
    def impact_assessor(self) -> ImpactAssessor:
        """Lazy initialization of impact assessor."""
        if not hasattr(self, '_impact_assessor') or self._impact_assessor is None:
            self._impact_assessor = ImpactAssessor()
        return self._impact_assessor

    @property
    def anomaly_detector(self) -> AnomalyDetector:
        """Lazy initialization of anomaly detector."""
        if not hasattr(self, '_anomaly_detector') or self._anomaly_detector is None:
            self._anomaly_detector = AnomalyDetector()
        return self._anomaly_detector
```

**Replace _assess_impact method:**
```python
    def _assess_impact(
        self,
        fact: Dict[str, Any],
        investigation_id: str,
    ) -> tuple[ImpactTier, Optional[str]]:
        """
        Assess impact tier based on geopolitical significance.

        Per CONTEXT.md: Impact based on entity significance AND event type.
        Investigation-relative: same fact may be critical in one investigation.

        Returns:
            (impact_tier, reasoning)
        """
        # Get investigation context if available (future enhancement)
        investigation_context = None  # Would come from investigation metadata

        result = self.impact_assessor.assess(fact, investigation_context)

        return result.tier, result.reasoning
```

**Replace _detect_dubious method:**
```python
    def _detect_dubious(
        self,
        fact: Dict[str, Any],
        credibility_score: float,
    ) -> tuple[list[DubiousFlag], list]:
        """
        Detect dubious flags using Boolean logic gates.

        Per CONTEXT.md taxonomy:
        - PHANTOM: hop_count > 2 AND primary_source IS NULL
        - FOG: claim_clarity < 0.5 OR attribution ~= "sources say"
        - ANOMALY: contradiction_count > 0
        - NOISE: source_credibility < 0.3

        Returns:
            (dubious_flags, classification_reasoning)
        """
        # Note: ANOMALY detection requires comparison facts
        # For single-fact classification, we skip ANOMALY (handled in batch)
        contradictions: List[Dict[str, Any]] = []

        result = self.dubious_detector.detect(fact, credibility_score, contradictions)

        # Convert ClassificationReasoning objects to dicts
        reasoning_dicts = [
            {
                "flag": r.flag.value,
                "reason": r.reason,
                "trigger_values": r.trigger_values,
            }
            for r in result.reasoning
        ]

        return result.flags, reasoning_dicts
```

**Add batch classification with anomaly detection:**
```python
    async def classify_investigation(
        self,
        investigation_id: str,
        facts: Optional[List[Dict[str, Any]]] = None,
    ) -> List[Dict[str, Any]]:
        """
        Classify all facts in an investigation with anomaly detection.

        This method enables ANOMALY detection by comparing facts against each other.

        Args:
            investigation_id: Investigation identifier
            facts: Optional list of facts (fetches from store if None)

        Returns:
            List of FactClassification dicts
        """
        # Get facts if not provided
        if facts is None:
            facts = await self.fact_store.get_all_facts(investigation_id)

        if not facts:
            self.logger.warning(f"No facts to classify in {investigation_id}")
            return []

        classifications = []

        # First pass: detect contradictions
        contradiction_map: Dict[str, List[Dict[str, Any]]] = {}
        for fact in facts:
            fact_id = fact.get("fact_id", "unknown")
            contradictions = await self.anomaly_detector.find_contradictions(fact, facts)
            if contradictions:
                contradiction_map[fact_id] = [
                    {"fact_id": c.fact_id_b, "type": c.contradiction_type, "confidence": c.confidence}
                    for c in contradictions
                ]

        # Second pass: classify with contradiction info
        for fact in facts:
            fact_id = fact.get("fact_id", "unknown")

            try:
                # Compute credibility
                credibility_score, credibility_breakdown = self._compute_credibility(fact)

                # Assess impact
                impact_tier, impact_reasoning = self._assess_impact(fact, investigation_id)

                # Detect dubious (with contradictions)
                contradictions = contradiction_map.get(fact_id, [])
                result = self.dubious_detector.detect(
                    fact,
                    credibility_score,
                    contradictions if contradictions else None,
                )

                # Convert reasoning
                reasoning_dicts = [
                    {
                        "flag": r.flag.value,
                        "reason": r.reason,
                        "trigger_values": r.trigger_values,
                    }
                    for r in result.reasoning
                ]

                # Calculate priority
                priority_score = self._calculate_priority(
                    impact_tier, result.flags, credibility_score
                )

                classification = FactClassification(
                    fact_id=fact_id,
                    investigation_id=investigation_id,
                    impact_tier=impact_tier,
                    dubious_flags=result.flags,
                    priority_score=priority_score,
                    credibility_score=credibility_score,
                    credibility_breakdown=credibility_breakdown,
                    classification_reasoning=reasoning_dicts,
                    impact_reasoning=impact_reasoning,
                )

                classifications.append(classification.model_dump(mode="json"))

            except Exception as e:
                self.logger.error(f"Failed to classify fact {fact_id}: {e}")
                continue

        # Save all classifications
        if classifications:
            await self.classification_store.save_classifications(
                investigation_id,
                [FactClassification(**c) for c in classifications],
            )

        self.logger.info(
            f"Classified {len(classifications)} facts in {investigation_id}",
            with_anomalies=len(contradiction_map),
        )

        return classifications
```
  </action>
  <verify>
```bash
uv run python -c "
import asyncio
from osint_system.agents.sifters import FactClassificationAgent
from osint_system.data_management.schemas import DubiousFlag, ImpactTier

async def test():
    agent = FactClassificationAgent()

    # Test full classification with various fact types
    facts = [
        # Critical + Dubious (PHANTOM)
        {
            'fact_id': 'f1',
            'claim': {'text': '[E1:Putin] ordered nuclear strike', 'claim_type': 'event'},
            'entities': [{'id': 'E1', 'text': 'Putin', 'type': 'PERSON', 'canonical': 'Vladimir Putin'}],
            'quality': {'extraction_confidence': 0.9, 'claim_clarity': 0.3},
            'provenance': {
                'source_id': 'aggregator.com',
                'source_type': 'unknown',
                'hop_count': 4,
                'offsets': {'start': 0, 'end': 100}
            }
        },
        # Critical + Clean
        {
            'fact_id': 'f2',
            'claim': {'text': '[E1:NATO] deployed troops to [E2:Poland]', 'claim_type': 'event'},
            'entities': [
                {'id': 'E1', 'text': 'NATO', 'type': 'ORGANIZATION'},
                {'id': 'E2', 'text': 'Poland', 'type': 'LOCATION'}
            ],
            'quality': {'extraction_confidence': 0.95, 'claim_clarity': 0.9},
            'provenance': {
                'source_id': 'https://reuters.com/news',
                'source_type': 'wire_service',
                'hop_count': 1,
                'source_classification': 'secondary',
                'attribution_chain': [{'entity': 'NATO spokesperson', 'hop': 0}],
                'offsets': {'start': 0, 'end': 100}
            }
        },
        # Less-critical + NOISE
        {
            'fact_id': 'f3',
            'claim': {'text': 'Some activity happened', 'claim_type': 'event'},
            'entities': [],
            'quality': {'extraction_confidence': 0.5, 'claim_clarity': 0.4},
            'provenance': {
                'source_id': 'random-blog.xyz',
                'source_type': 'unknown',
                'hop_count': 5,
                'offsets': {'start': 0, 'end': 50}
            }
        }
    ]

    # Test with sift (single-fact, no anomaly detection)
    results = await agent.sift({'facts': facts, 'investigation_id': 'int-test'})

    print(f'Classified {len(results)} facts')
    for r in results:
        print(f'  {r[\"fact_id\"]}: {r[\"impact_tier\"]}, flags={r[\"dubious_flags\"]}, cred={r[\"credibility_score\"]:.2f}, priority={r[\"priority_score\"]:.2f}')

    # Verify expectations
    f1 = next(r for r in results if r['fact_id'] == 'f1')
    assert f1['impact_tier'] == 'critical', 'f1 should be critical (Putin + nuclear)'
    assert DubiousFlag.PHANTOM.value in f1['dubious_flags'], 'f1 should have PHANTOM'
    assert DubiousFlag.FOG.value in f1['dubious_flags'], 'f1 should have FOG (low clarity)'

    f2 = next(r for r in results if r['fact_id'] == 'f2')
    assert f2['impact_tier'] == 'critical', 'f2 should be critical (NATO + troops)'
    assert len(f2['dubious_flags']) == 0, 'f2 should be clean'

    f3 = next(r for r in results if r['fact_id'] == 'f3')
    assert f3['impact_tier'] == 'less_critical', 'f3 should be less critical'
    assert DubiousFlag.NOISE.value in f3['dubious_flags'], 'f3 should have NOISE'

    print('Full integration OK')

asyncio.run(test())
"
```
  </verify>
  <done>FactClassificationAgent fully integrated with DubiousDetector, ImpactAssessor, AnomalyDetector</done>
</task>

<task type="auto">
  <name>Task 4: Add ImpactAssessor and AnomalyDetector tests</name>
  <files>
    tests/agents/sifters/classification/test_impact_assessor.py
    tests/agents/sifters/classification/test_anomaly_detector.py
  </files>
  <action>
Create comprehensive tests for ImpactAssessor and AnomalyDetector.

**test_impact_assessor.py:**
Test cases:
1. Critical: world leader entity
2. Critical: military action keywords
3. Critical: nuclear/missile keywords
4. Less-critical: routine fact
5. Entity significance scoring
6. Event type categorization
7. Investigation context boost
8. Bulk assessment

**test_anomaly_detector.py:**
Test cases:
1. Negation contradiction detection
2. Statement vs denial contradiction
3. Numeric range contradiction
4. Temporal conflict detection
5. No contradiction (unrelated facts)
6. Shared entity requirement
7. Confidence thresholds
  </action>
  <verify>
```bash
uv run python -m pytest tests/agents/sifters/classification/test_impact_assessor.py tests/agents/sifters/classification/test_anomaly_detector.py -v
```
  </verify>
  <done>All ImpactAssessor and AnomalyDetector tests pass</done>
</task>

</tasks>

<verification>
```bash
# All imports work
uv run python -c "
from osint_system.agents.sifters.classification import (
    DubiousDetector, ImpactAssessor
)
from osint_system.agents.sifters.classification.anomaly_detector import AnomalyDetector
print('All classification imports OK')
"

# Tests pass
uv run python -m pytest tests/agents/sifters/classification/ -v

# Full pipeline test
uv run python -c "
import asyncio
from osint_system.agents.sifters import FactClassificationAgent
from osint_system.data_management.schemas import DubiousFlag

async def full_test():
    agent = FactClassificationAgent()

    # Create test facts that exercise all paths
    facts = [
        # Should be: CRITICAL + PHANTOM + FOG
        {
            'fact_id': 'phantom-fog',
            'claim': {'text': 'Sources suggest Putin may have ordered something'},
            'entities': [{'text': 'Putin', 'canonical': 'Vladimir Putin'}],
            'quality': {'claim_clarity': 0.3},
            'provenance': {'hop_count': 5, 'source_type': 'unknown', 'offsets': {}}
        },
        # Should be: CRITICAL + clean
        {
            'fact_id': 'critical-clean',
            'claim': {'text': 'NATO announced military deployment'},
            'entities': [{'text': 'NATO'}],
            'quality': {'claim_clarity': 0.9},
            'provenance': {
                'source_id': 'reuters.com',
                'source_type': 'wire_service',
                'hop_count': 1,
                'source_classification': 'primary',
                'attribution_chain': [{'entity': 'NATO', 'hop': 0}],
                'offsets': {}
            }
        },
        # Should be: LESS_CRITICAL + NOISE
        {
            'fact_id': 'noise',
            'claim': {'text': 'Meeting happened'},
            'entities': [],
            'provenance': {'source_id': 'blog.xyz', 'hop_count': 3, 'offsets': {}}
        }
    ]

    results = await agent.sift({'facts': facts, 'investigation_id': 'final-test'})

    # Verify results
    phantom_fog = next(r for r in results if r['fact_id'] == 'phantom-fog')
    assert phantom_fog['impact_tier'] == 'critical'
    assert DubiousFlag.PHANTOM.value in phantom_fog['dubious_flags']
    assert DubiousFlag.FOG.value in phantom_fog['dubious_flags']
    print(f'phantom-fog: {phantom_fog[\"dubious_flags\"]} - OK')

    critical_clean = next(r for r in results if r['fact_id'] == 'critical-clean')
    assert critical_clean['impact_tier'] == 'critical'
    assert len(critical_clean['dubious_flags']) == 0
    print(f'critical-clean: {critical_clean[\"dubious_flags\"]} - OK')

    noise = next(r for r in results if r['fact_id'] == 'noise')
    assert noise['impact_tier'] == 'less_critical'
    assert DubiousFlag.NOISE.value in noise['dubious_flags']
    print(f'noise: {noise[\"dubious_flags\"]} - OK')

    # Verify priority ordering (critical dubious > critical clean > noise)
    assert phantom_fog['priority_score'] > 0, 'Critical dubious should have positive priority'
    assert critical_clean['priority_score'] == 0, 'Clean should have 0 priority (no verification needed)'
    assert noise['priority_score'] == 0, 'Noise should have 0 priority (batch only)'

    print('Full pipeline verification OK')

asyncio.run(full_test())
"
```
</verification>

<success_criteria>
- ImpactAssessor considers entity significance AND event type
- Critical entities include world leaders, military, NATO
- Critical events include military action, treaties, sanctions
- AnomalyDetector detects negation, numeric, temporal, attribution contradictions
- FactClassificationAgent integrates all components
- classify_investigation enables full anomaly detection
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-fact-classification-system/07-04-SUMMARY.md`
</output>
