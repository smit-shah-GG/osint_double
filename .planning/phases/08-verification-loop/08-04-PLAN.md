---
phase: 08-verification-loop
plan: 04
type: execute
wave: 3
depends_on: ["08-02", "08-03"]
files_modified:
  - osint_system/agents/sifters/verification/verification_agent.py
  - osint_system/agents/sifters/verification/search_executor.py
  - osint_system/agents/sifters/verification/__init__.py
  - osint_system/data_management/verification_store.py
  - tests/agents/sifters/verification/test_verification_agent.py
  - tests/data_management/test_verification_store.py
autonomous: true
user_setup:
  - service: serper
    why: "Search API for verification queries"
    env_vars:
      - name: SERPER_API_KEY
        source: "https://serper.dev -> Dashboard -> API Key"
    dashboard_config: []

must_haves:
  truths:
    - "VerificationAgent processes facts in parallel batches of 5-10 concurrent"
    - "Each fact gets up to 3 query attempts before UNVERIFIABLE status"
    - "CRITICAL tier facts require human review before finalization"
    - "Progress updates emitted for each verified fact (structlog)"
    - "Verification flows automatically from classification (no manual trigger)"
  artifacts:
    - path: "osint_system/agents/sifters/verification/verification_agent.py"
      provides: "Core verification agent with batch processing"
      exports: ["VerificationAgent"]
      min_lines: 350
    - path: "osint_system/agents/sifters/verification/search_executor.py"
      provides: "Search API integration"
      exports: ["SearchExecutor"]
      min_lines: 100
    - path: "osint_system/data_management/verification_store.py"
      provides: "Verification result storage"
      exports: ["VerificationStore"]
      min_lines: 120
    - path: "tests/agents/sifters/verification/test_verification_agent.py"
      provides: "Agent tests"
      min_lines: 150
  key_links:
    - from: "osint_system/agents/sifters/verification/verification_agent.py"
      to: "osint_system/agents/sifters/verification/query_generator.py"
      via: "query generation"
      pattern: "QueryGenerator"
    - from: "osint_system/agents/sifters/verification/verification_agent.py"
      to: "osint_system/agents/sifters/verification/evidence_aggregator.py"
      via: "evidence evaluation"
      pattern: "EvidenceAggregator"
    - from: "osint_system/agents/sifters/verification/verification_agent.py"
      to: "osint_system/agents/sifters/verification/reclassifier.py"
      via: "status transitions"
      pattern: "Reclassifier"
    - from: "osint_system/agents/sifters/verification/verification_agent.py"
      to: "osint_system/llm/rate_limiter.py"
      via: "API throttling"
      pattern: "RateLimiter"
---

<objective>
Build VerificationAgent with LangGraph orchestration, batch processing, and full integration.

Purpose: Orchestrate the complete verification loop - pull dubious facts from priority queue, generate queries, execute searches, aggregate evidence, reclassify facts, and update stores. Per CONTEXT.md: parallel batches of 5-10, automatic pipeline, human-in-the-loop for CRITICAL.

Output: Working VerificationAgent that processes dubious facts through targeted verification with progress reporting, SearchExecutor for API integration, VerificationStore for persistence, and comprehensive tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-verification-loop/08-CONTEXT.md
@.planning/phases/08-verification-loop/08-RESEARCH.md
@.planning/phases/08-verification-loop/08-01-SUMMARY.md
@.planning/phases/08-verification-loop/08-02-SUMMARY.md
@.planning/phases/08-verification-loop/08-03-SUMMARY.md
@osint_system/data_management/classification_store.py
@osint_system/data_management/fact_store.py
@osint_system/llm/rate_limiter.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement SearchExecutor and VerificationStore</name>
  <files>
    osint_system/agents/sifters/verification/search_executor.py
    osint_system/data_management/verification_store.py
  </files>
  <action>
Create search execution and storage infrastructure.

**SearchExecutor class (search_executor.py):**

```python
class SearchExecutor:
    """Execute verification searches using Serper API.

    Wraps langchain GoogleSerperAPIWrapper for search execution.
    Integrates with existing RateLimiter for throttling.
    Converts search results to EvidenceItem objects.
    """
```

**Constructor:**
- `rate_limiter: Optional[RateLimiter] = None` - Use existing RateLimiter
- `api_key: Optional[str] = None` - SERPER_API_KEY from env
- `max_results: int = 5` - Results per query
- `logger` - structlog bound to "SearchExecutor"

**Methods:**

1. `async def execute_query(self, query: VerificationQuery) -> List[EvidenceItem]`:
   - Wait for rate limiter if needed
   - Execute search via GoogleSerperAPIWrapper (or mock if no API key)
   - Convert results to EvidenceItem objects
   - Set source_url, source_domain, snippet from search result
   - Set authority_score via domain lookup
   - Set supports_claim = True initially (will be evaluated by evidence aggregator)
   - Set relevance_score based on keyword overlap with query

2. `async def execute_queries(self, queries: List[VerificationQuery]) -> List[EvidenceItem]`:
   - Execute queries sequentially (rate limited)
   - Return deduplicated evidence (by source_url)
   - Log each query execution

3. `_extract_domain(self, url: str) -> str`:
   - Extract domain from URL
   - Use urllib.parse or yarl (if available)

4. `_calculate_relevance(self, snippet: str, query: VerificationQuery) -> float`:
   - Simple keyword overlap score
   - Count query terms in snippet / total query terms
   - Return 0.0-1.0

**Handle missing API key gracefully:**
- Log warning "SERPER_API_KEY not set, using mock search"
- Return empty results in mock mode (verification will mark as UNVERIFIABLE)
- This allows tests to run without API key

**VerificationStore class (verification_store.py):**

```python
class VerificationStore:
    """Storage for verification results.

    Follows same pattern as ClassificationStore and FactStore.
    Investigation-scoped storage with investigation_id as primary key.
    """
```

**Constructor:**
- `persist_path: Optional[Path] = None` - JSON persistence (optional)
- `logger` - structlog bound to "VerificationStore"

**Data structure:**
```python
_results: Dict[str, Dict[str, VerificationResultRecord]]
# investigation_id -> fact_id -> result
```

**Methods:**

1. `async def save_result(self, result: VerificationResult) -> None`:
   - Create VerificationResultRecord from result
   - Store by investigation_id and fact_id
   - Optional: persist to JSON

2. `async def get_result(self, investigation_id: str, fact_id: str) -> Optional[VerificationResultRecord]`:
   - Return result if exists, None otherwise

3. `async def get_all_results(self, investigation_id: str) -> List[VerificationResultRecord]`:
   - Return all results for investigation

4. `async def get_by_status(self, investigation_id: str, status: VerificationStatus) -> List[VerificationResultRecord]`:
   - Filter results by status

5. `async def get_pending_review(self, investigation_id: str) -> List[VerificationResultRecord]`:
   - Return results where requires_human_review=True AND human_review_completed=False

6. `async def mark_reviewed(self, investigation_id: str, fact_id: str, notes: Optional[str] = None) -> None`:
   - Set human_review_completed=True
   - Set human_reviewer_notes if provided
  </action>
  <verify>
`uv run python -c "
from osint_system.agents.sifters.verification.search_executor import SearchExecutor
from osint_system.data_management.verification_store import VerificationStore
from osint_system.agents.sifters.verification.schemas import VerificationQuery, VerificationStatus
import asyncio

# Test SearchExecutor (mock mode without API key)
se = SearchExecutor()
query = VerificationQuery(
    query='test query',
    variant_type='entity_focused',
    target_sources=['news_outlet'],
    purpose='test',
    dubious_flag='phantom'
)
results = asyncio.run(se.execute_query(query))
print(f'Search results: {len(results)} (mock mode OK)')

# Test VerificationStore
vs = VerificationStore()
print('VerificationStore initialized')
"` succeeds
  </verify>
  <done>
SearchExecutor wraps Serper API with rate limiting and graceful mock mode, VerificationStore provides investigation-scoped result storage
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement VerificationAgent with batch processing</name>
  <files>
    osint_system/agents/sifters/verification/verification_agent.py
    osint_system/agents/sifters/verification/__init__.py
  </files>
  <action>
Create VerificationAgent with LangGraph-inspired verification loop and batch processing.

**VerificationAgent class:**

```python
class VerificationAgent(BaseSifter):
    """Orchestrates verification loop for dubious facts.

    Per CONTEXT.md:
    - Processes facts in parallel batches of 5-10 concurrent
    - Up to 3 query attempts per fact before UNVERIFIABLE
    - Human-in-the-loop for CRITICAL tier
    - Progress updates via structlog

    Verification flow per fact:
    1. Generate queries (QueryGenerator)
    2. Execute searches (SearchExecutor)
    3. Aggregate evidence (EvidenceAggregator)
    4. Reclassify (Reclassifier)
    5. Store result (VerificationStore)
    """
```

**Constructor:**
- `classification_store: Optional[ClassificationStore] = None`
- `fact_store: Optional[FactStore] = None`
- `verification_store: Optional[VerificationStore] = None`
- `batch_size: int = 10` - Per CONTEXT.md: 5-10 concurrent
- `max_query_attempts: int = 3` - Per CONTEXT.md
- Lazy-init components: QueryGenerator, SearchExecutor, EvidenceAggregator, Reclassifier
- `logger` - structlog bound to "VerificationAgent"

**Core method:**
```python
async def verify_investigation(
    self,
    investigation_id: str,
    progress_callback: Optional[Callable[[VerificationResult], Awaitable[None]]] = None,
) -> Dict[str, Any]:
    """Verify all dubious facts for an investigation.

    Per CONTEXT.md:
    1. Get priority queue from ClassificationStore
    2. Process in batches of batch_size
    3. Emit progress updates for each fact
    4. Return summary stats

    Returns:
        {
            "investigation_id": str,
            "total_verified": int,
            "confirmed": int,
            "refuted": int,
            "unverifiable": int,
            "superseded": int,
            "pending_review": int,  # CRITICAL tier awaiting human review
        }
    """
```

**Single-fact verification:**
```python
async def _verify_fact(
    self,
    fact_id: str,
    classification: FactClassification,
    investigation_id: str,
) -> VerificationResult:
    """Verify a single fact through query-search-evaluate loop.

    Per CONTEXT.md 3-query limit:
    1. Generate queries for dubious flags
    2. Execute each query until evidence sufficient OR all 3 exhausted
    3. Evaluate evidence
    4. Reclassify based on evaluation
    5. Store result
    """
```

**Batch processing:**
```python
async def _process_batch(
    self,
    classifications: List[FactClassification],
    investigation_id: str,
    progress_callback: Optional[Callable],
) -> List[VerificationResult]:
    """Process a batch of facts with controlled concurrency.

    Uses asyncio.Semaphore(batch_size) to limit concurrent verifications.
    Uses asyncio.gather with return_exceptions=True for resilience.
    """
```

**Implementation details:**

1. **Priority queue integration:**
   ```python
   queue = await self.classification_store.get_priority_queue(investigation_id)
   # Queue excludes NOISE-only facts per Phase 7
   ```

2. **Batch loop:**
   ```python
   for i in range(0, len(queue), self.batch_size):
       batch = queue[i:i + self.batch_size]
       results = await self._process_batch(batch, investigation_id, progress_callback)
       # Update stats
   ```

3. **Semaphore for concurrency:**
   ```python
   semaphore = asyncio.Semaphore(self.batch_size)

   async def verify_with_semaphore(classification):
       async with semaphore:
           return await self._verify_fact(...)
   ```

4. **Query loop with 3-attempt limit:**
   ```python
   queries = await self.query_generator.generate_queries(fact, classification)
   evidence = []
   for attempt, query in enumerate(queries[:3], 1):
       query_evidence = await self.search_executor.execute_query(query)
       evidence.extend(query_evidence)
       evaluation = await self.evidence_aggregator.evaluate_evidence(fact, evidence)
       if evaluation.status in (VerificationStatus.CONFIRMED, VerificationStatus.REFUTED):
           break  # Short-circuit on definitive result
   ```

5. **CRITICAL human review:**
   ```python
   if classification.impact_tier == ImpactTier.CRITICAL:
       result.requires_human_review = True
       # Don't finalize in ClassificationStore until reviewed
   ```

6. **Progress reporting:**
   ```python
   self._logger.info(
       "fact_verified",
       fact_id=result.fact_id,
       status=result.status.value,
       confidence_boost=result.confidence_boost,
       query_attempts=result.query_attempts,
   )
   if progress_callback:
       await progress_callback(result)
   ```

**Update __init__.py:**
Export VerificationAgent, SearchExecutor, and update full exports list.
  </action>
  <verify>
`uv run python -c "
from osint_system.agents.sifters.verification import VerificationAgent, QueryGenerator, EvidenceAggregator, Reclassifier, SearchExecutor
print('All verification components imported')

# Quick integration test
agent = VerificationAgent()
print(f'Agent batch_size: {agent.batch_size}')
print(f'Agent max_query_attempts: {agent.max_query_attempts}')
"` succeeds
  </verify>
  <done>
VerificationAgent processes facts in parallel batches, integrates all components (QueryGenerator, SearchExecutor, EvidenceAggregator, Reclassifier), respects 3-query limit
  </done>
</task>

<task type="auto">
  <name>Task 3: Add comprehensive tests</name>
  <files>
    tests/agents/sifters/verification/test_verification_agent.py
    tests/data_management/test_verification_store.py
  </files>
  <action>
Create comprehensive tests for VerificationAgent and VerificationStore.

**test_verification_agent.py:**

Test categories (pytest, mock external dependencies):

1. **Initialization tests:**
   - Default batch_size is 10
   - Default max_query_attempts is 3
   - Components lazy-initialized

2. **Single fact verification tests (mock search):**
   - PHANTOM fact generates source-chain queries
   - FOG fact generates clarity-seeking queries
   - ANOMALY fact generates compound queries
   - Query limit of 3 respected
   - Short-circuits on CONFIRMED
   - Short-circuits on REFUTED

3. **Batch processing tests:**
   - Batches respect batch_size
   - Parallel execution (verify timing)
   - Handles exceptions in batch without failing all
   - Progress callback called for each fact

4. **Status transition tests:**
   - High-authority evidence -> CONFIRMED
   - Refuting evidence -> REFUTED
   - No evidence after 3 queries -> UNVERIFIABLE
   - ANOMALY resolution -> SUPERSEDED or REFUTED based on type

5. **Human review tests:**
   - CRITICAL tier sets requires_human_review=True
   - LESS_CRITICAL does not require review
   - Pending review counted in summary

6. **Integration tests (with mock stores):**
   - Full verify_investigation flow
   - Stats calculated correctly
   - Results stored in VerificationStore

Use mocks:
- Mock SearchExecutor to return controlled evidence
- Mock ClassificationStore.get_priority_queue() to return test data
- Mock FactStore.get_fact() to return test facts

**test_verification_store.py:**

Test categories:

1. **Save and retrieve:**
   - Save result, retrieve by fact_id
   - Get all results for investigation
   - Get by status filter

2. **Human review tracking:**
   - get_pending_review returns only pending
   - mark_reviewed updates flags
   - Notes stored correctly

3. **Edge cases:**
   - Get from empty store returns None
   - Get from wrong investigation returns None

Target: 30+ tests for agent, 15+ tests for store.
  </action>
  <verify>
`uv run python -m pytest tests/agents/sifters/verification/test_verification_agent.py tests/data_management/test_verification_store.py -v` passes with 45+ tests
  </verify>
  <done>
Comprehensive tests cover verification loop, batch processing, status transitions, human review, and storage
  </done>
</task>

</tasks>

<verification>
1. `uv run python -c "from osint_system.agents.sifters.verification import VerificationAgent"` succeeds
2. `uv run python -m pytest tests/agents/sifters/verification/ -v` all tests pass
3. `uv run python -m pytest tests/data_management/test_verification_store.py -v` all tests pass
4. Batch processing uses asyncio.Semaphore for controlled parallelism
5. 3-query limit enforced with short-circuit on definitive result
6. CRITICAL tier facts have requires_human_review=True
7. Progress logs emitted for each verified fact
</verification>

<success_criteria>
- VerificationAgent.verify_investigation() processes entire priority queue
- Parallel batches of 5-10 concurrent verifications
- 3-query limit per fact before UNVERIFIABLE
- Short-circuits on CONFIRMED or REFUTED (doesn't waste queries)
- CRITICAL tier facts require human review
- Progress updates via structlog for each fact
- All results stored in VerificationStore
- 45+ tests pass covering agent and store
- Graceful handling when SERPER_API_KEY not set (mock mode)
</success_criteria>

<output>
After completion, create `.planning/phases/08-verification-loop/08-04-SUMMARY.md`
</output>
