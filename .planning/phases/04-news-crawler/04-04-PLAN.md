---
phase: 04-news-crawler
plan: 04
type: execute
---

<objective>
Create data routing to storage system and integrate with the broader agent ecosystem.

Purpose: Connect the news crawler to the data pipeline and enable communication with Planning Agent.
Output: Complete news crawler integrated with storage and message bus for agent collaboration.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-news-crawler/04-CONTEXT.md
@.planning/phases/04-news-crawler/04-03-SUMMARY.md
@osint_system/agents/crawlers/newsfeed_agent.py
@osint_system/data_management/data_store.py
@osint_system/communication/message_bus.py
@osint_system/agents/planning_agent.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement data storage adapter for articles</name>
  <files>osint_system/data_management/article_store.py</files>
  <action>Create ArticleStore class for persisting fetched articles. For beta, use in-memory storage with optional JSON persistence. Implement save_articles method that stores deduplicated articles with full metadata. Add retrieve_by_investigation method to fetch articles for specific investigation ID. Include timestamp-based retrieval for finding recent articles. Structure storage with investigation_id as primary key and articles as nested list. Implement basic indexing by URL for fast duplicate checks.</action>
  <verify>python -c "from osint_system.data_management.article_store import ArticleStore; store = ArticleStore(); store.save_articles('test_inv', []); print('Storage working')"</verify>
  <done>Article storage with investigation-based organization</done>
</task>

<task type="auto">
  <name>Task 2: Connect NewsFeedAgent to message bus</name>
  <files>osint_system/agents/crawlers/newsfeed_agent.py</files>
  <action>Update NewsFeedAgent to register with MessageBus and handle investigation requests. Subscribe to 'investigation.start' and 'crawler.fetch' topics. On receiving investigation request, trigger fetch_investigation_data with query parameters. After fetching and deduplication, publish results to 'crawler.complete' topic with article count and metadata statistics. Include investigation_id in all messages for correlation. Implement async message handlers using the existing A2A patterns from Phase 2-3.</action>
  <verify>python -c "from osint_system.agents.crawlers.newsfeed_agent import NewsFeedAgent; agent = NewsFeedAgent(); assert hasattr(agent, 'handle_message'); print('Message handling ready')"</verify>
  <done>NewsFeedAgent integrated with message bus for A2A communication</done>
</task>

<task type="auto">
  <name>Task 3: Create crawler coordination for Planning Agent</name>
  <files>osint_system/agents/planning_agent.py, tests/integration/test_crawler_integration.py</files>
  <action>Update PlanningAgent to include crawler task generation. When investigation starts, create 'Fetch news articles' task and publish to NewsFeedAgent. Modify task queue to handle crawler completion events and update investigation status. Create integration test that verifies end-to-end flow: Planning Agent creates investigation → sends to NewsFeedAgent → crawler fetches articles → stores in ArticleStore → notifies Planning Agent → ready for sifter agents. Test with mock RSS feed to avoid external dependencies.</action>
  <verify>python -m pytest tests/integration/test_crawler_integration.py -v</verify>
  <done>Complete integration between Planning and Crawler agents with tests</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] Articles persist to storage with full metadata
- [ ] NewsFeedAgent responds to investigation requests
- [ ] Planning Agent can trigger crawler execution
- [ ] Integration test demonstrates full pipeline
- [ ] Phase 4 complete, ready for Phase 5
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Storage system operational
- Agent communication working
- Integration tests passing
- Phase 4 complete
</success_criteria>

<output>
After completion, create `.planning/phases/04-news-crawler/04-04-SUMMARY.md`:

# Phase 4 Plan 4: Data Routing and Integration Summary

**Connected news crawler to storage and integrated with Planning Agent ecosystem**

## Accomplishments

- Article storage with investigation-based organization
- NewsFeedAgent message bus integration
- Planning Agent crawler coordination
- End-to-end integration testing
- Complete news crawler pipeline operational

## Files Created/Modified

- `osint_system/data_management/article_store.py` - Article persistence layer
- `osint_system/agents/crawlers/newsfeed_agent.py` - Message bus integration
- `osint_system/agents/planning_agent.py` - Crawler task generation
- `tests/integration/test_crawler_integration.py` - Integration tests

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Phase 4 complete. Ready for Phase 5: Extended Crawler Cohort (social media and documents)
</output>