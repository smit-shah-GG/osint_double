---
phase: 04-news-crawler
plan: 03
type: execute
---

<objective>
Add relevance filtering and comprehensive metadata extraction for better data quality and downstream processing.

Purpose: Implement smart deduplication and extract all metadata needed for credibility assessment and fact verification.
Output: Enhanced crawler with semantic deduplication and complete metadata preservation.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-news-crawler/04-CONTEXT.md
@.planning/phases/04-news-crawler/04-RESEARCH.md
@.planning/phases/04-news-crawler/04-02-SUMMARY.md
@osint_system/agents/crawlers/newsfeed_agent.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement semantic deduplication with SemHash</name>
  <files>osint_system/agents/crawlers/deduplication/dedup_engine.py</files>
  <action>Create DeduplicationEngine using three-layer strategy. Layer 1: URL-based deduplication using set membership. Layer 2: Content hash deduplication for exact matches using SHA256. Layer 3: Semantic deduplication with SemHash library (threshold 0.85) to catch rephrased content. Implement deduplicate_articles method that processes batch of articles and returns unique ones. Track seen URLs and hashes in memory for session persistence. Include statistics on duplicates found at each layer.</action>
  <verify>python -c "from osint_system.agents.crawlers.deduplication.dedup_engine import DeduplicationEngine; engine = DeduplicationEngine(); print('Dedup engine ready')"</verify>
  <done>Three-layer deduplication engine with semantic similarity detection</done>
</task>

<task type="auto">
  <name>Task 2: Extract and normalize comprehensive metadata</name>
  <files>osint_system/agents/crawlers/extractors/metadata_parser.py</files>
  <action>Create MetadataParser to extract and normalize article metadata. Extract source credibility (from config), temporal context (published date, retrieval time, age), geographic context (location mentions, source origin), author information (names, credentials if available), and article category/tags. Parse OpenGraph and Schema.org metadata from HTML when available. Normalize dates to UTC ISO format. Detect and tag article type (breaking news, analysis, opinion, etc). Calculate content metrics (word count, reading time).</action>
  <verify>python -c "from osint_system.agents.crawlers.extractors.metadata_parser import MetadataParser; parser = MetadataParser(); print('Metadata parser ready')"</verify>
  <done>Metadata parser extracting source, temporal, geographic, and content metadata</done>
</task>

<task type="auto">
  <name>Task 3: Integrate deduplication and metadata into fetch pipeline</name>
  <files>osint_system/agents/crawlers/newsfeed_agent.py</files>
  <action>Update NewsFeedAgent.fetch_investigation_data to include full pipeline: fetch from sources, extract article content, parse metadata, apply three-layer deduplication, return deduplicated articles with complete metadata. Add exhaustive retrieval mode that returns everything relevant regardless of age (per CONTEXT requirement). Include deduplication statistics in response. Preserve all metadata throughout pipeline for downstream agents. Structure output as list of Article objects with all fields populated.</action>
  <verify>python -c "from osint_system.agents.crawlers.newsfeed_agent import NewsFeedAgent; agent = NewsFeedAgent(); print('Full pipeline integrated')"</verify>
  <done>Complete fetch pipeline with deduplication and metadata preservation</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] Semantic deduplication correctly identifies similar articles
- [ ] Metadata includes source credibility and temporal context
- [ ] Geographic context extracted when present
- [ ] Exhaustive retrieval mode returns all relevant content
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Deduplication working at all three layers
- Complete metadata extracted
- Ready for downstream processing
</success_criteria>

<output>
After completion, create `.planning/phases/04-news-crawler/04-03-SUMMARY.md`:

# Phase 4 Plan 3: Relevance Filtering and Metadata Extraction Summary

**Enhanced crawler with semantic deduplication and comprehensive metadata extraction**

## Accomplishments

- Three-layer deduplication (URL, hash, semantic)
- SemHash integration for semantic similarity
- Complete metadata extraction and normalization
- Exhaustive retrieval mode for thorough coverage
- Full pipeline integration

## Files Created/Modified

- `osint_system/agents/crawlers/deduplication/dedup_engine.py` - Deduplication engine
- `osint_system/agents/crawlers/extractors/metadata_parser.py` - Metadata extraction
- `osint_system/agents/crawlers/newsfeed_agent.py` - Enhanced with full pipeline

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for 04-04-PLAN.md (Data routing to storage)
</output>