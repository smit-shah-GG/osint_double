---
phase: 01-foundation-environment
plan: 03
type: execute
---

<objective>
Integrate Gemini API with rate limiting and token tracking.

Purpose: Establish robust API connection with production-ready safeguards against rate limits.
Output: Working Gemini client with exponential backoff, token counting, and simple test endpoint.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-environment/01-CONTEXT.md
@.planning/phases/01-foundation-environment/01-RESEARCH.md
@.planning/phases/01-foundation-environment/01-01-SUMMARY.md
@.planning/phases/01-foundation-environment/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Gemini client with rate limiting</name>
  <files>osint_system/llm/__init__.py, osint_system/llm/gemini_client.py</files>
  <action>
    Create llm package with __init__.py.
    Create gemini_client.py with GeminiClient class:
    Import google.generativeai as genai, time, random, functools, loguru.logger.
    Import settings from config.settings.
    Class GeminiClient:
      __init__(): Configure genai with API key from settings, create model instance (settings.gemini_model).
      _exponential_backoff decorator: Implement retry with exponential backoff + jitter.
      Max 5 retries, base delay 1.0s, exponential increase (2^retry), jitter 0-10% of delay.
      Log each retry attempt.
      @_exponential_backoff generate_content(prompt, temperature=0.7):
        Call model.generate_content(), return response.text.
        Handle google.generativeai.types.generation_types.BlockedPromptException.
      count_tokens(text): Use model.count_tokens(text).result.total_tokens.
    Export client = GeminiClient() singleton.
  </action>
  <verify>uv run python -c "from osint_system.llm.gemini_client import client; print(client.count_tokens('test'))" returns token count</verify>
  <done>Gemini client implemented with exponential backoff, error handling, and token counting</done>
</task>

<task type="auto">
  <name>Task 2: Create rate limiter and test endpoint</name>
  <files>osint_system/llm/rate_limiter.py, osint_system/cli/main.py</files>
  <action>
    Create rate_limiter.py with TokenBucket class:
    Import time, threading.
    Class TokenBucket:
      __init__(capacity=15, refill_rate=0.25): Initialize with tokens=capacity, lock=threading.Lock().
      acquire(tokens=1): Thread-safe token acquisition.
      With lock: if tokens >= requested, deduct and return True, else return False.
      _refill(): Calculate tokens to add based on time elapsed, cap at capacity.
    Create RateLimiter class using TokenBucket:
      __init__(): Create buckets for RPM (15) and TPM (1_000_000).
      can_proceed(token_count): Check both RPM and TPM buckets.

    Update cli/main.py:
    Add @app.command() test_gemini():
      Import gemini_client.
      Prompt user for test prompt.
      Display token count.
      Call generate_content with prompt.
      Display response truncated to 500 chars.
      Show success message with timing.
  </action>
  <verify>uv run python -m osint_system.cli.main test-gemini prompts for input and returns Gemini response</verify>
  <done>Rate limiter implemented with token bucket algorithm, test endpoint added to CLI</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] Gemini API key loads from environment
- [ ] Rate limiter prevents exceeding 15 RPM
- [ ] Exponential backoff handles 429 errors gracefully
- [ ] Token counting works accurately
- [ ] Test command generates responses
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Can make successful API calls
- Rate limiting prevents quota exceeded errors
- Retry logic handles transient failures
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-environment/01-03-SUMMARY.md`:

# Phase 1 Plan 3: Gemini API Integration Summary

**Integrated Gemini API with production-ready rate limiting and error handling.**

## Accomplishments

- Implemented exponential backoff with jitter for API resilience
- Created token bucket rate limiter for RPM/TPM management
- Added token counting for cost monitoring
- Built test endpoint in CLI

## Files Created/Modified

- `osint_system/llm/gemini_client.py` - Gemini API client wrapper
- `osint_system/llm/rate_limiter.py` - Token bucket rate limiting
- `osint_system/cli/main.py` - Added test-gemini command

## Decisions Made

- Exponential backoff with 5 retries max
- Token bucket algorithm for rate limiting
- Singleton pattern for client instance

## Issues Encountered

None - API integration working as designed.

## Next Step

Ready for 01-04-PLAN.md (Basic Agent Proof-of-Concept)
</output>